{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmanuelbillions88/Text-Clustering-Pub/blob/main/IlB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42d395e"
      },
      "source": [
        "# Task\n",
        "Implement a Topic Clustering model using LangGraph with nodes for Data Cleaning (spaCy), Vector Extraction (bge-large-en-v1.5), Dimensionality Reduction (UMAP), Clustering (HDBSACN & K-Means), Cluster Naming (Gemini 1.5 Flash and KeyBERT), and Storage (ChromaDB).\n",
        "The model should use HDBSACN if the numberof clusters is specified and use K-Means for clustering when the number of clusters is specified. The cluster names should be a maximum of 5 words. Include comments in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7953b6cc"
      },
      "source": [
        "## Set up the langgraph environment\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries and define the graph structure with nodes for each stage (Data Cleaning, Vector Extraction, Dimensionality Reduction, Clustering, Cluster Naming, Storage, Visualization) and an orchestrator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c903c3a"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to enter in the sample_data and install the necessary libraries for the topic clustering model. This involves installing spaCy for data cleaning, sentence-transformers for embedding, umap-learn for dimensionality reduction, scikit-learn and hdbscan for clustering, keybert for cluster naming, and chromadb for storage, along with langgraph for building the graph.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "50a4826c"
      },
      "outputs": [],
      "source": [
        "# @title # Dependencies\n",
        "!pip install --quiet langgraph\n",
        "!pip install --quiet -U sentence-transformers\n",
        "!pip install --quiet nltk\n",
        "!pip install --quiet sklearn\n",
        "!pip install --quiet transformers\n",
        "!pip install --quiet spacy\n",
        "!pip install --quiet umap-learn\n",
        "!pip install --quiet hdbscan\n",
        "!pip install --quiet hdbscan\n",
        "!pip install --quiet keybert\n",
        "!pip install --quiet chromadb\n",
        "!pip install --quiet\n",
        "\n",
        "!python -m --quiet spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bDz7clVJ7Uua"
      },
      "outputs": [],
      "source": [
        "# @title # Extrating Texts from .txt files\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "import json\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load model and prepare text\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "file_path= '/content/Drilling Technology and Well Completion.txt'\n",
        "long_text = \"\"\n",
        "\n",
        "try:\n",
        "  with open(file_path, 'r') as f:\n",
        "    long_text = f.read()\n",
        "except FileNotFoundError:\n",
        "  print(\"File not found. Make sure 'your_file.txt' is in the correct directory.\")\n",
        "\n",
        "sentences = sent_tokenize(long_text)\n",
        "\n",
        "# Step 1: Encode sentences\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# Step 2: Cluster sentences semantically\n",
        "# Adjust n_clusters or distance_threshold as needed\n",
        "clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.0, metric='euclidean', linkage='ward')\n",
        "labels = clustering.fit_predict(embeddings)\n",
        "\n",
        "# Step 3: Group sentences by cluster label\n",
        "clustered_chunks = {}\n",
        "for sentence, label in zip(sentences, labels):\n",
        "    clustered_chunks.setdefault(label, []).append(sentence)\n",
        "\n",
        "# Step 4: Convert clusters to word-based chunks\n",
        "chunks = [\" \".join(sent_list) for sent_list in clustered_chunks.values()]\n",
        "\n",
        "# Result: chunks is a list of semantically grouped text chunks (strings)\n",
        "sample_data = []\n",
        "for i, chunk in enumerate(chunks):\n",
        "    sample_data.append(chunk)\n",
        "\n",
        "print(\"\\n\", f\"sample_data entered successfully!: {len(sample_data)} sentences.\", json.dumps([f'{i+1}) {sentence}' for i, sentence in enumerate(sample_data[:5])], indent=4), sep=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBV7DGYd7LlJ"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary libraries are installed, I need to import the models from the libaries so they can be used in my LangGraph Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E4ReSDiWBch0"
      },
      "outputs": [],
      "source": [
        "# @title # Importing Libaries from Dependencies\n",
        "# Import models from LIbaries\n",
        "from langgraph.graph import StateGraph, END\n",
        "import typing\n",
        "from typing import List, Optional, Dict, Any\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap\n",
        "from sklearn.cluster import KMeans # Keep KMeans for optional use\n",
        "import hdbscan # Import HDBSCAN\n",
        "from keybert import KeyBERT\n",
        "from collections import defaultdict\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import chromadb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "print(\"Models imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e901b4a1"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary libraries are imported, I need to define the state of the graph, instantiate the StateGraph, define the nodes representing each stage of the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gaLZ0qA5BkyY"
      },
      "outputs": [],
      "source": [
        "# @title #Langraph Attributes\n",
        "# After defining GraphState in the previous cell\n",
        "class GraphState(typing.TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        input_data: Original input data (list of strings).\n",
        "        cleaned_data: Data after cleaning (list of strings).\n",
        "        embeddings: Embeddings of the cleaned data (numpy array).\n",
        "        reduced_embeddings: Dimensionality-reduced embeddings (numpy array).\n",
        "        cluster_labels: Labels assigned to each data point (list of ints).\n",
        "        cluster_names: Names generated for each cluster (dictionary).\n",
        "        num_clusters: Optional number of clusters for K-Means (int).\n",
        "        storage_status: Indicates if storage is complete (string).\n",
        "        visualization_status: Indicates if visualization is complete (string).\n",
        "        error: Any error encountered during the process (string).\n",
        "        next_node: Explicitly set next node for orchestrator routing (string).\n",
        "    \"\"\"\n",
        "    input_data: List[str]\n",
        "    cleaned_data: Optional[List[str]]\n",
        "    embeddings: Optional[np.ndarray]\n",
        "    reduced_embeddings: Optional[np.ndarray] # Corrected type hint\n",
        "    cluster_labels: Optional[List[int]]\n",
        "    cluster_names: Optional[Dict[int, str]]\n",
        "    storage_status: Optional[str]\n",
        "    visualization_status: Optional[str]\n",
        "    num_clusters: Optional[int]\n",
        "    error: Optional[str]\n",
        "    next_node: Optional[str]\n",
        "\n",
        "print(\"LangGraph Attributes Defined Successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94c56604"
      },
      "source": [
        "**Reasoning**:\n",
        "Defining the Data Cleaning Node function\n",
        "\n",
        "Import spaCy model for text cleaning and processing (Lowercasing, Punctuation and Stop Word reomval)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "BBE32W2qB6Mn"
      },
      "outputs": [],
      "source": [
        "# @title # Data Cleaning Node\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the Data Cleaning Node function\n",
        "def clean_data(state: GraphState) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Cleans the input text data using spaCy.\n",
        "\n",
        "    This function performs tokenization, lowercasing, removes punctuation and stop words,\n",
        "    and lemmatizes the tokens to produce cleaned text.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cleaned_data.\n",
        "        Returns an error in the state if input_data is missing.\n",
        "    \"\"\"\n",
        "    print(\"---DATA CLEANING NODE---\")\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "\n",
        "\n",
        "    if input_data is None:\n",
        "        print(\"Error: No input data available for cleaning.\")\n",
        "        return {\"error\": \"No input data available for cleaning.\"}\n",
        "\n",
        "    cleaned_texts = []\n",
        "\n",
        "    for i, text in enumerate(input_data):\n",
        "        if isinstance(text, str): # Ensure the input is a string\n",
        "\n",
        "            if (i==0):\n",
        "                concat_text = text + \" \" + input_data[i+1]\n",
        "            elif (i==len(input_data) - 1):\n",
        "                concat_text = input_data[i-1] + \" \" + text\n",
        "            elif (i>0 and i<(len(input_data) - 1)):\n",
        "                concat_text = input_data[i-1] + \" \" + text + \" \" + input_data[i+1]\n",
        "\n",
        "             # Process text with spaCy\n",
        "            doc = nlp(concat_text)\n",
        "\n",
        "            # Tokenization, lowercasing, punctuation removal, stop word removal, and lemmatization\n",
        "            cleaned_text = \" \".join([\n",
        "                token.lemma_.lower() for token in doc\n",
        "                if not token.is_punct and not token.is_stop and not token.is_space\n",
        "            ])\n",
        "            cleaned_texts.append(cleaned_text)\n",
        "        else:\n",
        "            print(f\"Warning: Skipping non-string input: {text}\")\n",
        "\n",
        "\n",
        "    print(f\"Cleaned {len(cleaned_texts)} texts.\")\n",
        "    print(f\"First cleaned text sample: {cleaned_texts[:1]}\") # Debugging print\n",
        "    print(f\"Returning state update: {{'cleaned_data': ...}}\") # Debugging print\n",
        "\n",
        "    return {\"cleaned_data\": cleaned_texts}\n",
        "\n",
        "print(\"Data Cleaning Node implemented and added to the workflow.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avmMVBIqAxx9"
      },
      "source": [
        "**Reasoning**:\n",
        "Defining the Vector Extraction Node function\n",
        "\n",
        "Importing bge-large-en-v1.5 model for extracting vector embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "fhTdEmrkCRDc"
      },
      "outputs": [],
      "source": [
        "# @title # Vector Embeddings Extarction Node\n",
        "# Load a pre-trained sentence transformer model for generating embeddings\n",
        "# Replacing 'intfloat/es-large-v2' with 'bge-large-en-v1.5' as requested\n",
        "try:\n",
        "    embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
        "    print(\"Loaded embedding model: BAAI/bge-large-en-v1.5\")\n",
        "    print(\"Vector Extraction Node implemented and added to the workflow.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embedding model 'BAAI/bge-large-en-v1.5': {e}\")\n",
        "    # Handle this error appropriately, maybe return an error state\n",
        "    raise e # Re-raise the exception if the model fails to load\n",
        "\n",
        "\n",
        "# Define the Vector Extraction Node function\n",
        "def extract_embeddings(state: GraphState) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extracts vector embeddings from cleaned text data using the selected pre-trained model.\n",
        "\n",
        "    This function uses the loaded SentenceTransformer model to encode the cleaned text\n",
        "    into numerical vector representations.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with cleaned_data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with embeddings (numpy array).\n",
        "        Returns an error in the state if cleaned_data is missing.\n",
        "    \"\"\"\n",
        "    print(\"---VECTOR EXTRACTION NODE (Updated)---\")\n",
        "    cleaned_data = state.get(\"cleaned_data\") # Use .get() for safer access\n",
        "\n",
        "    if cleaned_data is None:\n",
        "        print(\"Error: No cleaned data available for embedding.\")\n",
        "        return {\"error\": \"No cleaned data available for embedding.\"}\n",
        "\n",
        "    print(f\"Extracting embeddings for {len(cleaned_data)} lines of texts using the updated model...\")\n",
        "    # Generate embeddings using the new model\n",
        "    embeddings = embedding_model.encode(cleaned_data)\n",
        "    print(\"Embeddings extraction complete (Updated).\")\n",
        "    return {\"embeddings\": embeddings} # Ensure this returns a dictionary to update state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLgUROXvAyAo"
      },
      "source": [
        "**Reasoning**:\n",
        "Defining the Dimensionality Reduction Node function\n",
        "\n",
        "Importing UMAP for reducing the dimensionality of the vector embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "jvZxUjqyCqA3"
      },
      "outputs": [],
      "source": [
        "# @title # Dimensionality Reduction Node\n",
        "# Define the Dimensionality Reduction Node function\n",
        "def reduce_dimensionality(state: GraphState) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Reduces the dimensionality of vector embeddings using UMAP.\n",
        "\n",
        "    This step is crucial for visualizing high-dimensional embeddings and can\n",
        "    also improve the performance of clustering algorithms by removing noise\n",
        "    and highlighting meaningful structures. The target dimensionality is\n",
        "    determined based on the number of input samples.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with reduced_embeddings (numpy array).\n",
        "        Returns an error in the state if embeddings or input_data are missing.\n",
        "    \"\"\"\n",
        "    print(\"---DIMENSIONALITY REDUCTION NODE---\")\n",
        "    embeddings = state.get(\"embeddings\") # Use .get() for safer access\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "\n",
        "    if embeddings is None:\n",
        "        print(\"Error: No embeddings available for dimensionality reduction.\")\n",
        "        return {\"error\": \"No embeddings available for dimensionality reduction.\"}\n",
        "\n",
        "    if input_data is None:\n",
        "        print(\"Error: Input data is missing, cannot determine dimensionality.\")\n",
        "        return {\"error\": \"Input data is missing, cannot determine dimensionality.\"}\n",
        "\n",
        "    n_samples = len(input_data)\n",
        "    # Determine target dimensionality based on the number of samples\n",
        "    if n_samples <= 500:\n",
        "        n_components = 20\n",
        "    elif n_samples <= 5000:\n",
        "        n_components = 30\n",
        "    elif n_samples <= 20000:\n",
        "        n_components = 50\n",
        "    else:\n",
        "        n_components = 100\n",
        "\n",
        "    print(f\"Reducing dimensionality to {n_components} using UMAP...\")\n",
        "    # Initialize and fit UMAP\n",
        "    # Using default parameters for simplicity, but these can be tuned\n",
        "    reducer = umap.UMAP(n_components=n_components, random_state=42)\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "    print(\"Dimensionality reduction complete.\")\n",
        "    return {\"reduced_embeddings\": reduced_embeddings}\n",
        "\n",
        "print(\"Dimensionality Reduction Node implemented and added to the workflow.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lztLyGWpAyC8"
      },
      "source": [
        "**Reasoning**:\n",
        "Defining the Data Clustering Node function\n",
        "\n",
        "Import HDBSCAN & K-Means for Clustering so we can use HDBSCAN if no_cluster == None and use K-Means if no_cluster == int()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Y4_axeLSDBkf"
      },
      "outputs": [],
      "source": [
        "# @title # Topic Clustering Node\n",
        "# Define the Clustering Node function\n",
        "def cluster_data(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Clusters the dimensionality-reduced data using HDBSCAN or K-Means.\n",
        "\n",
        "    If `num_clusters` is specified (any integer), K-Means is used for clustering.\n",
        "    If `num_clusters` is None, HDBSCAN is used.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with reduced_embeddings and optional num_clusters.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cluster_labels (list of ints) or an error message.\n",
        "    \"\"\"\n",
        "    print(\"---CLUSTERING NODE---\")\n",
        "    reduced_embeddings = state.get(\"reduced_embeddings\") # Use .get() for safer access\n",
        "    num_clusters = state.get(\"num_clusters\")\n",
        "\n",
        "    if reduced_embeddings is None:\n",
        "        print(\"Error: No reduced embeddings available for clustering.\")\n",
        "        return {\"error\": \"No reduced embeddings available for clustering.\"}\n",
        "\n",
        "    cluster_labels = None\n",
        "\n",
        "    # Use K-Means if num_clusters is specified (any integer)\n",
        "    if num_clusters is not None and isinstance(num_clusters, int) and num_clusters > 0:\n",
        "        print(f\"Applying K-Means to achieve {num_clusters} clusters...\")\n",
        "        try:\n",
        "            kmeans_model = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "            cluster_labels = kmeans_model.fit_predict(reduced_embeddings)\n",
        "            print(\"K-Means clustering complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during K-Means clustering: {e}\")\n",
        "            return {\"error\": f\"K-Means clustering failed: {e}\"}\n",
        "\n",
        "    # Use HDBSCAN if num_clusters is None\n",
        "    elif num_clusters is None:\n",
        "        print(\"Performing clustering using HDBSCAN...\")\n",
        "        # Use HDBSCAN to find clusters and identify noise points\n",
        "        # Adjust min_cluster_size and min_samples as needed for your data\n",
        "        hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=5) # Default parameters\n",
        "        cluster_labels = hdbscan_model.fit_predict(reduced_embeddings)\n",
        "        n_noise = list(cluster_labels).count(-1)\n",
        "        print(f\"HDBSCAN found {len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)} clusters and {n_noise} noise points.\")\n",
        "        print(\"HDBSCAN clustering complete.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"Invalid value for num_clusters: {num_clusters}. Please provide an integer > 0 for K-Means or None for HDBSCAN.\")\n",
        "        return {\"error\": f\"Invalid value for num_clusters: {num_clusters}\"}\n",
        "\n",
        "\n",
        "    if cluster_labels is not None:\n",
        "        return {\"cluster_labels\": cluster_labels.tolist()} # Ensure labels are a list for JSON compatibility\n",
        "    else:\n",
        "        return {\"error\": \"Clustering failed to produce labels.\"}\n",
        "\n",
        "\n",
        "print(\"Clustering Node implemented and added to the workflow.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Alx_QfaL6iT"
      },
      "source": [
        "**Reasoning**:\n",
        "Defining the Cluster Naming Node function\n",
        "\n",
        "Importing Gemini 1.5 Flash‑Lite and KeyBERT for assigning semantic names to clusters with a maximum of 5 words. Gemini 1.5 Flash‑Lite is the main model for naming clusters while KeyBERT is a fallback model if Gemini 1.5 Flash‑Lite fails"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QL1OhcceDJum"
      },
      "outputs": [],
      "source": [
        "# @title # Reclustering Node (Cluster Naming)\n",
        "# Load a pre-trained KeyBERT model (still useful for keyword suggestions if needed)\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Configure Gemini API for cluster naming\n",
        "try:\n",
        "    # Assuming GOOGLE_API_KEY is already set in the environment or Colab secrets\n",
        "    GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest') # Using a suitable model for naming\n",
        "    print(\"Gemini API configured successfully.\")\n",
        "    print(\"Clustering Naming Successful\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API: {e}\")\n",
        "    gemini_model = None # Set to None if configuration fails\n",
        "\n",
        "\n",
        "def get_closest_to_centroid(embeddings, labels, cluster_id):\n",
        "    \"\"\"\n",
        "    Finds the embedding closest to the centroid of a given cluster.\n",
        "\n",
        "    Args:\n",
        "        embeddings: A numpy array of embeddings.\n",
        "        labels: A numpy array of cluster labels corresponding to the embeddings.\n",
        "        cluster_id: The ID of the cluster to process.\n",
        "\n",
        "    Returns:\n",
        "        The embedding vector that is closest to the centroid of the cluster.\n",
        "    \"\"\"\n",
        "    print(\"Embs: \", embeddings.shape, \"labels: \", labels, \"cluster_label\", cluster_id, flush=True)\n",
        "    # Select embeddings belonging to the specified cluster\n",
        "    cluster_embeddings = embeddings[labels == cluster_id]\n",
        "\n",
        "    print(f\"Processing cluster {cluster_id} with {len(cluster_embeddings)} embeddings from {len(embeddings)} embeddings.\", flush=True)\n",
        "\n",
        "    if cluster_embeddings.shape[0] == 0:\n",
        "        return None  # Handle empty clusters\n",
        "\n",
        "    # Calculate the centroid of the cluster\n",
        "    centroid = np.mean(cluster_embeddings, axis=0)\n",
        "\n",
        "    print(\"Centroid Dim: \", centroid.shape, flush=True)\n",
        "\n",
        "    # Calculate the Euclidean distance from each embedding in the cluster to the centroid\n",
        "    distances = np.linalg.norm(cluster_embeddings - centroid, axis=1)\n",
        "\n",
        "    print(\"Distances: \", distances, flush=True)\n",
        "\n",
        "    # Find the index of the embedding with the minimum distance\n",
        "    closest_embedding_index_in_cluster = np.argmin(distances)\n",
        "\n",
        "    print(\"Closest embedding index: \", closest_embedding_index_in_cluster, flush=True)\n",
        "\n",
        "    # Get the embedding vector at that index\n",
        "    centre_embedding = cluster_embeddings[closest_embedding_index_in_cluster]\n",
        "\n",
        "    print(\"Closest embedding: \", centre_embedding, flush=True)\n",
        "\n",
        "    # get index of center embedding\n",
        "    embedding_index = np.where((embeddings == centre_embedding).all(axis=1))[0][0]\n",
        "\n",
        "    print(\"Closest embedding index: \", embedding_index, flush=True)\n",
        "\n",
        "    return embedding_index\n",
        "\n",
        "\n",
        "# Define the Cluster Naming Node function\n",
        "def name_clusters(state: GraphState) -> Dict[str, Dict[int, str]]:\n",
        "    \"\"\"\n",
        "    Names the clusters using Gemini API or KeyBERT, after re-clustering within each initial cluster.\n",
        "\n",
        "    This function first groups documents by their initial cluster labels. For each cluster,\n",
        "    it performs K-Means re-clustering with 5 random centroids. It then uses the documents\n",
        "    within these re-clustered groups to generate a concise, semantic name for the\n",
        "    *original* cluster using either the Gemini API (if configured) or KeyBERT. It handles\n",
        "    potential API failures by falling back to KeyBERT and ensures names are a maximum of 5 words.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data and cluster_labels.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cluster_names (dictionary mapping original cluster ID to name).\n",
        "        Returns an error in the state if input_data or cluster_labels are missing.\n",
        "    \"\"\"\n",
        "    print(\"---CLUSTER NAMING NODE (Refined with Re-clustering)---\")\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "    cluster_labels = state.get(\"cluster_labels\") # Use .get() for safer access\n",
        "    reduced_embeddings = state.get(\"reduced_embeddings\") # Need embeddings for re-clustering\n",
        "\n",
        "    if input_data is None or cluster_labels is None or reduced_embeddings is None:\n",
        "        print(\"Error: Input data, cluster labels, or reduced embeddings are missing for naming.\")\n",
        "        return {\"error\": \"Input data, cluster labels, or reduced embeddings are missing for naming.\"}\n",
        "\n",
        "    # Group documents and their embeddings by initial cluster label\n",
        "    clustered_data = defaultdict(lambda: {\"docs\": [], \"embeddings\": []})\n",
        "    for doc, label, embedding in zip(input_data, cluster_labels, reduced_embeddings):\n",
        "        clustered_data[label][\"docs\"].append(doc)\n",
        "        clustered_data[label][\"embeddings\"].append(embedding)\n",
        "\n",
        "    cluster_names = {}\n",
        "    # Generate a name for each initial cluster\n",
        "    for cluster_id, data in clustered_data.items():\n",
        "        docs = data[\"docs\"]\n",
        "        embeddings = np.array(data[\"embeddings\"])\n",
        "\n",
        "        if cluster_id == -1:\n",
        "            cluster_names[cluster_id] = \"Noise\"\n",
        "            continue\n",
        "\n",
        "        if not docs:\n",
        "            cluster_names[cluster_id] = \"Empty Cluster\"\n",
        "            continue\n",
        "\n",
        "        cluster_name = None # Initialize cluster_name to None\n",
        "\n",
        "        # Perform K-Means re-clustering within the current cluster\n",
        "        print(\"Docs: \", len(docs), flush=True)\n",
        "        n_sub_clusters = min(5, len(docs)) # Re-cluster into at most 5 sub-clusters\n",
        "        if n_sub_clusters > 1:\n",
        "            print(f\"Re-clustering within original Cluster {cluster_id} using K-Means ({n_sub_clusters} sub-clusters)...\")\n",
        "            try:\n",
        "                kmeans_sub_model = KMeans(n_clusters=n_sub_clusters, random_state=42, n_init=10)\n",
        "                sub_cluster_labels = kmeans_sub_model.fit_predict(embeddings)\n",
        "\n",
        "                # Select a representative document sample from each sub-cluster for naming\n",
        "                naming_docs = []\n",
        "\n",
        "                # Group documents by sub-cluster\n",
        "                sub_clustered_docs = defaultdict(list)\n",
        "                for doc, sub_label in zip(docs, sub_cluster_labels):\n",
        "                    naming_docs.append(docs[get_closest_to_centroid(embeddings, sub_cluster_labels, sub_label)])\n",
        "                    # sub_clustered_docs[sub_label].append(doc)\n",
        "\n",
        "\n",
        "                # for sub_label, sub_docs in sub_clustered_docs.items():\n",
        "                #     if sub_docs:\n",
        "                #          # Take a sample from each sub-cluster\n",
        "                #         sample_size_per_sub = max(1, len(sub_docs) // n_sub_clusters) # Adjust sample size based on sub-cluster size\n",
        "                #         naming_docs.extend(np.random.choice(sub_docs, size=min(sample_size_per_sub, len(sub_docs)), replace=False).tolist())\n",
        "\n",
        "                # Limit the total number of documents for naming to avoid exceeding context window\n",
        "                # naming_docs = naming_docs[:20] # Still limit the total documents for the prompt\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during K-Means re-clustering for Cluster {cluster_id}: {e}. Using original documents for naming.\")\n",
        "                naming_docs = docs[:20] # Fallback to using original docs sample\n",
        "\n",
        "        else:\n",
        "             # If only one document or sub-cluster not possible, use original docs sample\n",
        "             naming_docs = docs[:20]\n",
        "\n",
        "        # Use Gemini API for naming if configured\n",
        "        print(\"Naming docs: \", len(naming_docs), flush=True)\n",
        "        if gemini_model and naming_docs:\n",
        "            print(f\"Attempting to generate name for original Cluster {cluster_id} using Gemini API based on re-clustered samples...\")\n",
        "            # Refine the prompt to be more direct about the desired output format and constraints\n",
        "            naming_docs = [\"\\n\\n- \" + name_doc for name_doc in naming_docs]\n",
        "            prompt = f\"\"\"Analyze the following sample texts from a cluster and provide a concise name (maximum 5 words) that summarizes the main topic. Ensure the name is semantic and easy to understand.\n",
        "\n",
        "Texts:\n",
        "{''.join(naming_docs)}\n",
        "\n",
        "Concise Name (max 5 words):\"\"\"\n",
        "            print(\"Showing promt:\", prompt)\n",
        "            try:\n",
        "                response = gemini_model.generate_content(prompt)\n",
        "                if response and response.text:\n",
        "                    cluster_name_raw = response.text.strip()\n",
        "                    # Ensure the concise name is max 5 words\n",
        "                    cluster_name = \" \".join(cluster_name_raw.split()[:5])\n",
        "                    print(f\"Generated name for original Cluster {cluster_id} with Gemini API: {cluster_name}\")\n",
        "                else:\n",
        "                    print(f\"Gemini API returned an empty response for original Cluster {cluster_id}. Falling back to KeyBERT.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating name for original Cluster {cluster_id} with Gemini API: {e}. Falling back to KeyBERT.\")\n",
        "\n",
        "        # Fallback to KeyBERT if Gemini API failed or not configured, or if no naming docs\n",
        "        if cluster_name is None and (docs and naming_docs):\n",
        "            print(f\"Using KeyBERT for original Cluster {cluster_id}...\")\n",
        "            # Use a representative set of documents for KeyBERT\n",
        "            keybert_docs = naming_docs if naming_docs else docs[:50] # Use naming docs if available, otherwise a larger sample from original\n",
        "            cluster_text = \" \".join(keybert_docs)\n",
        "            keywords = kw_model.extract_keywords(\n",
        "                cluster_text,\n",
        "                keyphrase_ngram_range=(1, 3),\n",
        "                stop_words='english',\n",
        "                use_mmr=True,\n",
        "                diversity=0.7,\n",
        "                top_n=5\n",
        "            )\n",
        "            keyword_list = [keyword[0] for keyword in keywords]\n",
        "            # Combine keywords into a name, ensuring it's max 5 words\n",
        "            cluster_name = \" \".join(keyword_list).split()[:5]\n",
        "            cluster_name = \" \".join(cluster_name)\n",
        "\n",
        "            print(f\"Generated name for original Cluster {cluster_id} with KeyBERT: {cluster_name}\")\n",
        "        elif cluster_name is None:\n",
        "             cluster_name = \"Unnamed Cluster\"\n",
        "             print(f\"Could not generate name for original Cluster {cluster_id} using Gemini or KeyBERT.\")\n",
        "\n",
        "\n",
        "        cluster_names[cluster_id] = cluster_name\n",
        "\n",
        "\n",
        "    print(\"Cluster naming complete (Refined with Re-clustering).\")\n",
        "    return {\"cluster_names\": cluster_names} # Ensure this returns a dictionary to update state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMwh4OVZL6tU"
      },
      "source": [
        "**Reasoning**:\n",
        "Defining the Storage Node function\n",
        "\n",
        "Importing ChromaDB for storing clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "kkjHhk2xDVmw"
      },
      "outputs": [],
      "source": [
        "# @title # Storage Node\n",
        "# Load ChromaDB for storage\n",
        "client = chromadb.Client()\n",
        "# Uncomment to disable ChromaDB analytics\n",
        "# %env CHROMA_ANALYTICS=False\n",
        "\n",
        "# Define the Storage Node function\n",
        "def store_results(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Stores the clustered data and cluster names in ChromaDB.\n",
        "\n",
        "    This function creates a collection in ChromaDB and adds the original documents\n",
        "    along with their assigned cluster labels and names as metadata.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data, cluster_labels, and cluster_names.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary indicating the storage is complete or an error message.\n",
        "        Returns an error in the state if input_data, cluster_labels, or cluster_names are missing.\n",
        "    \"\"\"\n",
        "    print(\"---STORAGE NODE---\")\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "    cluster_labels = state.get(\"cluster_labels\") # Use .get() for safer access\n",
        "    cluster_names = state.get(\"cluster_names\") # Use .get() for safer access\n",
        "\n",
        "    if input_data is None or cluster_labels is None or cluster_names is None:\n",
        "        print(\"Error: Data, labels, or names are missing for storage.\")\n",
        "        return {\"error\": \"Data, labels, or names are missing for storage.\"}\n",
        "\n",
        "    # Create or get a collection\n",
        "    collection_name = \"topic_clusters\"\n",
        "    try:\n",
        "        # Attempt to delete collection if it exists to avoid issues with re-adding\n",
        "        client.delete_collection(name=collection_name)\n",
        "        print(f\"Deleted existing collection: {collection_name}\")\n",
        "    except:\n",
        "        pass # Ignore if collection doesn't exist\n",
        "\n",
        "    try:\n",
        "        collection = client.create_collection(name=collection_name)\n",
        "        print(f\"Created collection: {collection_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating collection: {e}\")\n",
        "        return {\"error\": f\"Error creating collection: {e}\"}\n",
        "\n",
        "\n",
        "    # Prepare data for ChromaDB\n",
        "    ids = [f\"doc_{i}\" for i in range(len(input_data))]\n",
        "    # Store original text and cluster label as metadata\n",
        "    metadatas = []\n",
        "    for i in range(len(input_data)):\n",
        "        metadata = {\"cluster_label\": str(cluster_labels[i])}\n",
        "        # Add cluster name to metadata if available\n",
        "        if cluster_labels[i] in cluster_names:\n",
        "            metadata[\"cluster_name\"] = cluster_names[cluster_labels[i]]\n",
        "        metadatas.append(metadata)\n",
        "\n",
        "\n",
        "    # Add data to the collection\n",
        "    # Note: ChromaDB requires embeddings for add, but we only need to store text and metadata for this task\n",
        "    # A workaround is to use the original embeddings or generate dummy ones if not available.\n",
        "    # For simplicity, we will store the original text as documents and metadata.\n",
        "    # If you need to query by similarity, you would store the embeddings here.\n",
        "    print(f\"Adding {len(input_data)} documents to ChromaDB collection '{collection_name}'...\")\n",
        "    try:\n",
        "        collection.add(\n",
        "            documents=input_data,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "        print(\"Storage complete.\")\n",
        "        return {\"storage_status\": \"complete\"}\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding documents to collection: {e}\")\n",
        "        return {\"error\": f\"Error adding documents to collection: {e}\"}\n",
        "\n",
        "print(\"Storage Node implemented and added to the workflow.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRYYvhpBL6v0"
      },
      "source": [
        "**Reasoning**:\n",
        "Defining the Visualization Node function\n",
        "\n",
        "Importing UMAP reducing dimensionality embeddings for visualization (Scatter plots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "zxzuyo7zDsoA"
      },
      "outputs": [],
      "source": [
        "# @title # Cluster Visualization Node (Scatter Plot)\n",
        "# Define the Visualization Node function\n",
        "def visualize_clusters(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Visualizes the clustered, dimensionality-reduced data using UMAP and cluster labels/names.\n",
        "\n",
        "    This function generates a scatter plot of the 2D dimensionality-reduced data,\n",
        "    coloring points by their cluster labels and adding a legend with cluster names.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with reduced_embeddings, cluster_labels, and cluster_names.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary indicating the visualization is complete or an error message.\n",
        "        Returns an error in the state if reduced_embeddings, cluster_labels, or cluster_names are missing.\n",
        "    \"\"\"\n",
        "    print(\"---VISUALIZATION NODE---\")\n",
        "    reduced_embeddings = state.get(\"reduced_embeddings\") # Use .get() for safer access\n",
        "    cluster_labels = state.get(\"cluster_labels\") # Use .get() for safer access\n",
        "    cluster_names = state.get(\"cluster_names\") # Use .get() for safer access\n",
        "    input_data = state.get(\"input_data\")\n",
        "\n",
        "    if reduced_embeddings is None or cluster_labels is None or cluster_names is None:\n",
        "        print(\"Error: Reduced embeddings, cluster labels, or cluster names are missing for visualization.\")\n",
        "        return {\"error\": \"Reduced embeddings, cluster labels, or cluster names are missing for visualization.\"}\n",
        "\n",
        "    # Ensure reduced_embeddings are in a plottable format (e.g., 2D)\n",
        "    if reduced_embeddings.shape[1] > 2:\n",
        "         print(\"Warning: Reduced embeddings are not 2D. Performing UMAP again for visualization.\")\n",
        "         try:\n",
        "            # Reduce to 2 components specifically for visualization\n",
        "            reducer_2d = umap.UMAP(n_components=3, random_state=42)\n",
        "            reduced_embeddings_2d = reducer_2d.fit_transform(reduced_embeddings)\n",
        "         except Exception as e:\n",
        "             print(f\"Error reducing dimensionality to 2D for visualization: {e}\")\n",
        "             return {\"error\": f\"Error reducing dimensionality to 2D for visualization: {e}\"}\n",
        "    else:\n",
        "        reduced_embeddings_2d = reduced_embeddings\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = sns.scatterplot(\n",
        "        x=reduced_embeddings_2d[:, 0],\n",
        "        y=reduced_embeddings_2d[:, 1],\n",
        "        z=reduced_embeddings_2d[:, 1],\n",
        "        hue=cluster_labels,\n",
        "        palette='viridis',\n",
        "        legend='full',\n",
        "        alpha=0.7\n",
        "    )\n",
        "\n",
        "    # Add cluster names as labels to the plot (optional, can be crowded)\n",
        "    # You might want to add labels only for cluster centroids or a sample of points\n",
        "    # For simplicity, let's use a legend with names\n",
        "    handles, labels = scatter.get_legend_handles_labels()\n",
        "    # Map numeric labels to cluster names for the legend\n",
        "    named_labels = [cluster_names.get(int(label), f\"Cluster {label}\") for label in labels]\n",
        "    plt.legend(handles, named_labels, title=\"Clusters\")\n",
        "\n",
        "\n",
        "    plt.title('Cluster Visualization (UMAP)')\n",
        "    plt.xlabel('UMAP Component 1')\n",
        "    plt.ylabel('UMAP Component 2')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Visualization complete.\")\n",
        "    return {\"visualization_status\": \"complete\"}\n",
        "\n",
        "print(\"Visualization Node inplemented and added to the workflow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92f76603"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the orchestrator function for controlling the flow based on the presence of the number of clusters parameter, set the entry point to the orchestrator, define the edges between the nodes, and set the end point.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "DO56FNUTD605"
      },
      "outputs": [],
      "source": [
        "# @title #Orchestrator Node\n",
        "# Define the Orchestrator Node function\n",
        "def orchestrator(state: GraphState) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Directs the workflow based on the current state and presence of errors.\n",
        "\n",
        "    This node acts as the central control, determining the next processing step\n",
        "    based on which stages have been completed or if an error has occurred.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with the next_node to execute or END.\n",
        "    \"\"\"\n",
        "    print(\"---ORCHESTRATOR NODE---\")\n",
        "    error = state.get(\"error\")\n",
        "    storage_status = state.get(\"storage_status\")\n",
        "    visualization_status = state.get(\"visualization_status\")\n",
        "\n",
        "    # If there's an error, stop the process\n",
        "    if error:\n",
        "        print(f\"Error detected: {error}. Stopping workflow.\")\n",
        "        return {\"next_node\": END, \"error\": error}\n",
        "\n",
        "    # Determine next step based on completed steps in sequence\n",
        "    # Check for the latest completed step first\n",
        "    if state.get(\"cleaned_data\") is None:\n",
        "        print(\"Proceeding to data cleaning.\")\n",
        "        return {\"next_node\": \"clean\"}\n",
        "    elif state.get(\"embeddings\") is None and state.get(\"cleaned_data\") is not None:\n",
        "        print(\"Proceeding to vector extraction.\")\n",
        "        return {\"next_node\": \"embed\"}\n",
        "    elif state.get(\"reduced_embeddings\") is None and state.get(\"embeddings\") is not None:\n",
        "        print(\"Proceeding to dimensionality reduction.\")\n",
        "        return {\"next_node\": \"reduce_dim\"}\n",
        "    elif state.get(\"cluster_labels\") is None and state.get(\"reduced_embeddings\") is not None:\n",
        "        print(\" Proceeding to clustering.\")\n",
        "        return {\"next_node\": \"cluster\"}\n",
        "    elif state.get(\"cluster_names\") is None and state.get(\"cluster_labels\") is not None:\n",
        "        print(\"Proceeding to cluster naming.\")\n",
        "        return {\"next_node\": \"name_clusters\"}\n",
        "    elif state.get(\"storage_status\") is None and state.get(\"cluster_names\") is not None:\n",
        "         print(\"Proceeding to storage.\")\n",
        "         return {\"next_node\": \"store\"}\n",
        "    elif visualization_status is None and state.get(\"cluster_names\") is not None:\n",
        "         print(\"Proceeding to visualization.\")\n",
        "         return {\"next_node\": \"visualize_clusters\"}\n",
        "    else:\n",
        "        print(\"All processing steps complete. Ending workflow.\")\n",
        "        return {\"next_node\": END}\n",
        "\n",
        "\n",
        "print(\"Orchestrator Node inplemented and added to the workflow\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qow85FMnL61v"
      },
      "source": [
        "**Reasoning**:\n",
        "This connects the LangGraph workflow together\n",
        "\n",
        "Run this cell to run the Topic Clustering Model built with LangGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7b09f153"
      },
      "outputs": [],
      "source": [
        "# @title # This is where the MAGIC happen!👇\n",
        "# Define the LangGraph workflow\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add nodes for each stage of the workflow\n",
        "workflow.add_node(\"clean\", clean_data) # Node for data cleaning\n",
        "workflow.add_node(\"embed\", extract_embeddings) # Node for vector embedding extraction\n",
        "workflow.add_node(\"reduce_dim\", reduce_dimensionality) # Node for dimensionality reduction\n",
        "workflow.add_node(\"cluster\", cluster_data) # Node for clustering\n",
        "workflow.add_node(\"name_clusters\", name_clusters) # Node for naming clusters\n",
        "workflow.add_node(\"store\", store_results) # Node for storing results in ChromaDB\n",
        "workflow.add_node(\"visualize_clusters\", visualize_clusters) # Node for visualizing clusters\n",
        "workflow.add_node(\"orchestrator\", orchestrator) # Node for directing the workflow\n",
        "\n",
        "\n",
        "# Set the entry point of the workflow to the orchestrator\n",
        "workflow.set_entry_point(\"orchestrator\")\n",
        "\n",
        "# Add conditional edges from the orchestrator\n",
        "# The orchestrator's return value (the string name of the next node or END)\n",
        "# will determine which node to execute next.\n",
        "workflow.add_conditional_edges(\n",
        "    \"orchestrator\",\n",
        "    lambda state: state.get(\"next_node\", \"clean\"), # Evaluate the state for the next node name\n",
        "    {\n",
        "        \"clean\": \"clean\", # If orchestrator returns \"clean\", go to the clean node\n",
        "        \"embed\": \"embed\", # If orchestrator returns \"embed\", go to the embed node\n",
        "        \"reduce_dim\": \"reduce_dim\", # If orchestrator returns \"reduce_dim\", go to the reduce_dim node\n",
        "        \"cluster\": \"cluster\", # If orchestrator returns \"cluster\", go to the cluster node\n",
        "        \"name_clusters\": \"name_clusters\", # If orchestrator returns \"name_clusters\", go to the name_clusters node\n",
        "        \"store\": \"store\", # If orchestrator returns \"store\", go to the store node\n",
        "        \"visualize_clusters\": \"visualize_clusters\", # If orchestrator returns \"visualize_clusters\", go to the visualize_clusters node\n",
        "        END: END # If orchestrator returns END, the workflow stops\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define the edges (transitions) between nodes\n",
        "# Each processing node transitions back to the orchestrator to decide the next step\n",
        "workflow.add_edge(\"clean\", \"orchestrator\")\n",
        "workflow.add_edge(\"embed\", \"orchestrator\")\n",
        "workflow.add_edge(\"reduce_dim\", \"orchestrator\")\n",
        "workflow.add_edge(\"cluster\", \"orchestrator\")\n",
        "workflow.add_edge(\"name_clusters\", \"orchestrator\")\n",
        "workflow.add_edge(\"store\", \"orchestrator\")\n",
        "workflow.add_edge(\"visualize_clusters\", \"orchestrator\") # After visualization, go back to orchestrator to potentially end\n",
        "\n",
        "\n",
        "# Define the final edge from the last processing node to END\n",
        "# This edge signifies the end of the workflow after visualization is complete.\n",
        "workflow.add_edge(\"visualize_clusters\", END)\n",
        "\n",
        "# Compile the workflow into a runnable application\n",
        "app = workflow.compile()\n",
        "\n",
        "# Define the initial inputs for the workflow\n",
        "# 'input_data' contains the text data to be processed.\n",
        "# 'num_clusters' is an optional parameter for the clustering node.\n",
        "# Setting 'num_clusters' to an integer will attempt to use K-Means with that number of clusters\n",
        "# Setting 'num_clusters' to None will use HDBSCAN clustering.\n",
        "# Setting 'num_clusters' to none will lead to an error. Use 'None' instead\n",
        "\n",
        "\n",
        "inputs = {\"input_data\": sample_data, \"num_clusters\": None}\n",
        "\n",
        "\n",
        "# Invoke the workflow with the defined inputs and store the final state\n",
        "final_state = app.invoke(inputs)\n",
        "\n",
        "print(\"\\n---Workflow Execution Complete---\")\n",
        "# You can further inspect the results from the final state, e.g., the cluster labels and names\n",
        "print(\"Cluster Labels:\", final_state.get(\"cluster_labels\"))\n",
        "print(\"Cluster Names:\", final_state.get(\"cluster_names\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "90bf34df"
      },
      "outputs": [],
      "source": [
        "# @title # Run this Cell to View the Clusters and their Contents\n",
        "# Get the cluster labels, cluster names, and original input data from the final state\n",
        "cluster_labels = final_state.get(\"cluster_labels\")\n",
        "cluster_names = final_state.get(\"cluster_names\")\n",
        "input_data = final_state.get(\"input_data\")\n",
        "\n",
        "if cluster_labels is None or cluster_names is None or input_data is None:\n",
        "    print(\"Cluster labels, names, or input data not found in the final state.\")\n",
        "else:\n",
        "    # Group documents by cluster label\n",
        "    clustered_docs = defaultdict(list)\n",
        "    for doc, label in zip(input_data, cluster_labels):\n",
        "        clustered_docs[label].append(doc)\n",
        "\n",
        "    # Print the contents of each cluster\n",
        "    print(\"\\n--- Cluster Contents ---\")\n",
        "    for cluster_id, docs in clustered_docs.items():\n",
        "        cluster_name = cluster_names.get(cluster_id, f\"Cluster {cluster_id}\")\n",
        "        print(f\"\\nCluster {cluster_id}: {cluster_name} (Count: {len(docs)})\")\n",
        "        for i, doc in enumerate(docs):\n",
        "            print(f\"  {i+1}. {doc}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}