{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmanuelbillions88/Text-Clustering-Pub/blob/main/IlB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42d395e"
      },
      "source": [
        "# Task\n",
        "Implement a topic clustering model using LangGraph with nodes for Data Cleaning (spaCy), Vector Extraction (DistilBERT), Dimensionality Reduction (UMAP), Clustering (OPTICS & K-Means), Cluster Naming (KeyBERT), and Storage (ChromaDB). The model should handle noise points with OPTICS and use K-Means for clustering when the number of clusters is specified. The cluster names should be a maximum of 5 words. Include comments in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7953b6cc"
      },
      "source": [
        "## Set up the langgraph environment\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries and define the graph structure with nodes for each stage (Data Cleaning, Vector Extraction, Dimensionality Reduction, Clustering, Cluster Naming, Storage) and an orchestrator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c903c3a"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to install the necessary libraries for the topic clustering model. This involves installing spaCy for data cleaning, sentence-transformers for embedding, umap-learn for dimensionality reduction, scikit-learn and hdbscan for clustering, keybert for cluster naming, and chromadb for storage, along with langgraph for building the graph.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "50a4826c",
        "outputId": "1a880231-6b3c-46c5-e4be-9b01fa5770bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "%pip install spacy sentence-transformers umap-learn scikit-learn hdbscan keybert chromadb langgraph --quiet\n",
        "%pip install langgraph --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7b09f153",
        "outputId": "a932bdff-8dbd-4dcb-f1a3-4527b09c45f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded embedding model: all-MiniLM-L6-v2\n",
            "Error configuring Gemini API: Requesting secret GOOGLE_API_KEY timed out. Secrets can only be fetched when running from the Colab UI.\n",
            "env: CHROMA_ANALYTICS=False\n",
            "---ORCHESTRATOR NODE---\n",
            "Cleaned data not found. Proceeding to data cleaning.\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "import typing\n",
        "from typing import List, Optional, Dict, Any\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap\n",
        "from sklearn.cluster import KMeans, OPTICS\n",
        "from keybert import KeyBERT\n",
        "from collections import defaultdict\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import chromadb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming GraphState is defined in a previous cell\n",
        "class GraphState(typing.TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        input_data: Original input data (list of strings).\n",
        "        cleaned_data: Data after cleaning (list of strings).\n",
        "        embeddings: Embeddings of the cleaned data (numpy array).\n",
        "        reduced_embeddings: Dimensionality-reduced embeddings (numpy array).\n",
        "        cluster_labels: Labels assigned to each data point (list of ints).\n",
        "        cluster_names: Names generated for each cluster (dictionary).\n",
        "        num_clusters: Optional number of clusters for K-Means (int).\n",
        "        error: Any error encountered during the process (string).\n",
        "        next_node: Explicitly set next node for orchestrator routing (string).\n",
        "        storage_status: Indicates if storage is complete (string).\n",
        "        visualization_status: Indicates if visualization is complete (string).\n",
        "    \"\"\"\n",
        "    input_data: List[str]\n",
        "    cleaned_data: Optional[List[str]] = None\n",
        "    embeddings: Optional[Any] = None\n",
        "    reduced_embeddings: Optional[Any] = None\n",
        "    cluster_labels: Optional[List[int]] = None\n",
        "    cluster_names: Optional[Dict[int, str]] = None\n",
        "    num_clusters: Optional[int] = None\n",
        "    error: Optional[str] = None\n",
        "    next_node: Optional[str] = None\n",
        "    storage_status: Optional[str] = None\n",
        "    visualization_status: Optional[str] = None\n",
        "\n",
        "\n",
        "# Load spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the Data Cleaning Node function\n",
        "def clean_data(state: GraphState) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Cleans the input text data using spaCy.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cleaned_data.\n",
        "    \"\"\"\n",
        "    print(\"---DATA CLEANING NODE---\")\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "\n",
        "    if input_data is None:\n",
        "        print(\"Error: No input data available for cleaning.\")\n",
        "        return {\"error\": \"No input data available for cleaning.\"}\n",
        "\n",
        "    cleaned_texts = []\n",
        "\n",
        "    for text in input_data:\n",
        "        if isinstance(text, str): # Ensure the input is a string\n",
        "             # Process text with spaCy\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Tokenization, lowercasing, punctuation removal, stop word removal, and lemmatization\n",
        "            cleaned_text = \" \".join([\n",
        "                token.lemma_.lower() for token in doc\n",
        "                if not token.is_punct and not token.is_stop and not token.is_space\n",
        "            ])\n",
        "            cleaned_texts.append(cleaned_text)\n",
        "        else:\n",
        "            print(f\"Warning: Skipping non-string input: {text}\")\n",
        "\n",
        "\n",
        "    print(f\"Cleaned {len(cleaned_texts)} texts.\")\n",
        "    print(f\"First cleaned text sample: {cleaned_texts[:1]}\") # Debugging print\n",
        "    print(f\"Returning state update: {{'cleaned_data': ...}}\") # Debugging print\n",
        "\n",
        "    return {\"cleaned_data\": cleaned_texts}\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "# Using 'all-MiniLM-L6-v2' as a reliable general-purpose model\n",
        "try:\n",
        "    new_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(\"Loaded embedding model: all-MiniLM-L6-v2\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embedding model 'all-MiniLM-L6-v2': {e}\")\n",
        "    # Handle this error appropriately, maybe return an error state\n",
        "    raise e # Re-raise the exception if the fallback also fails\n",
        "\n",
        "\n",
        "# Define the Vector Extraction Node function\n",
        "def extract_embeddings(state: GraphState) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extracts vector embeddings from cleaned text data using the selected pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with cleaned_data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with embeddings.\n",
        "    \"\"\"\n",
        "    print(\"---VECTOR EXTRACTION NODE (Updated)---\")\n",
        "    cleaned_data = state.get(\"cleaned_data\") # Use .get() for safer access\n",
        "\n",
        "    if cleaned_data is None:\n",
        "        print(\"Error: No cleaned data available for embedding.\")\n",
        "        return {\"error\": \"No cleaned data available for embedding.\"}\n",
        "\n",
        "    print(f\"Extracting embeddings for {len(cleaned_data)} texts using the updated model...\")\n",
        "    # Generate embeddings using the new model\n",
        "    embeddings = new_model.encode(cleaned_data)\n",
        "    print(\"Embeddings extraction complete (Updated).\")\n",
        "    return {\"embeddings\": embeddings} # Ensure this returns a dictionary to update state\n",
        "\n",
        "# Define the Dimensionality Reduction Node function\n",
        "def reduce_dimensionality(state: GraphState) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Reduces the dimensionality of vector embeddings using UMAP.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with reduced_embeddings.\n",
        "    \"\"\"\n",
        "    print(\"---DIMENSIONALITY REDUCTION NODE---\")\n",
        "    embeddings = state.get(\"embeddings\") # Use .get() for safer access\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "\n",
        "    if embeddings is None:\n",
        "        print(\"Error: No embeddings available for dimensionality reduction.\")\n",
        "        return {\"error\": \"No embeddings available for dimensionality reduction.\"}\n",
        "\n",
        "    if input_data is None:\n",
        "        print(\"Error: Input data is missing, cannot determine dimensionality.\")\n",
        "        return {\"error\": \"Input data is missing, cannot determine dimensionality.\"}\n",
        "\n",
        "    n_samples = len(input_data)\n",
        "    # Determine target dimensionality based on the number of samples\n",
        "    if n_samples <= 500:\n",
        "        n_components = 20\n",
        "    elif n_samples <= 5000:\n",
        "        n_components = 30\n",
        "    elif n_samples <= 20000:\n",
        "        n_components = 50\n",
        "    else:\n",
        "        n_components = 100\n",
        "\n",
        "    print(f\"Reducing dimensionality to {n_components} using UMAP...\")\n",
        "    # Initialize and fit UMAP\n",
        "    reducer = umap.UMAP(n_components=n_components, random_state=42)\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "    print(\"Dimensionality reduction complete.\")\n",
        "    return {\"reduced_embeddings\": reduced_embeddings}\n",
        "\n",
        "\n",
        "# Define the Clustering Node function\n",
        "def cluster_data(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Clusters the dimensionality-reduced data using K-Means with n_clusters=2\n",
        "    when num_clusters is set to 2 in the state, ensuring no noise points.\n",
        "    Retains OPTICS logic for other num_clusters values or when num_clusters is not provided.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with reduced_embeddings and optional num_clusters.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cluster_labels or an error message.\n",
        "    \"\"\"\n",
        "    print(\"---CLUSTERING NODE---\")\n",
        "    reduced_embeddings = state.get(\"reduced_embeddings\") # Use .get() for safer access\n",
        "    num_clusters = state.get(\"num_clusters\")\n",
        "\n",
        "    if reduced_embeddings is None:\n",
        "        print(\"Error: No reduced embeddings available for clustering.\")\n",
        "        return {\"error\": \"No reduced embeddings available for clustering.\"}\n",
        "\n",
        "    cluster_labels = None\n",
        "\n",
        "    # If num_clusters is specifically 2, use K-Means on all data points\n",
        "    if num_clusters == 2:\n",
        "        print(f\"Applying K-Means to achieve exactly {num_clusters} clusters on all data points...\")\n",
        "        try:\n",
        "            kmeans_model = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "            cluster_labels = kmeans_model.fit_predict(reduced_embeddings)\n",
        "            print(\"K-Means clustering complete (2 clusters, no noise).\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during K-Means clustering: {e}\")\n",
        "            return {\"error\": f\"K-Means clustering failed: {e}\"}\n",
        "\n",
        "    # Otherwise, use OPTICS or K-Means on non-noise points if num_clusters is specified and not 2\n",
        "    else:\n",
        "        print(\"Performing clustering using OPTICS...\")\n",
        "        # Use OPTICS to find clusters and identify noise points\n",
        "        optics_model = OPTICS(min_samples=10, xi=0.05, min_cluster_size=0.05)\n",
        "        optics_model.fit(reduced_embeddings)\n",
        "\n",
        "        optics_labels = optics_model.labels_\n",
        "        noise_points = optics_labels == -1\n",
        "        n_noise = list(optics_labels).count(-1)\n",
        "\n",
        "        print(f\"OPTICS found {len(set(optics_labels)) - (1 if -1 in optics_labels else 0)} clusters and {n_noise} noise points.\")\n",
        "\n",
        "        if num_clusters is not None and num_clusters > 0:\n",
        "            print(f\"Applying K-Means to achieve {num_clusters} clusters on non-noise points...\")\n",
        "            # Filter out noise points for K-Means\n",
        "            non_noise_indices = np.where(~noise_points)[0]\n",
        "            non_noise_embeddings = reduced_embeddings[non_noise_indices]\n",
        "\n",
        "            if len(non_noise_embeddings) == 0:\n",
        "                print(\"Warning: No non-noise points to apply K-Means.\")\n",
        "                # Assign -1 to all points if no non-noise points\n",
        "                final_cluster_labels = np.full(len(reduced_embeddings), -1, dtype=int)\n",
        "            elif num_clusters > len(non_noise_embeddings):\n",
        "                 print(f\"Warning: Requested number of clusters ({num_clusters}) is greater than the number of non-noise points ({len(non_noise_embeddings)}). Using OPTICS labels.\")\n",
        "                 final_cluster_labels = optics_labels\n",
        "            else:\n",
        "                # Apply K-Means\n",
        "                kmeans_model = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "                kmeans_labels = kmeans_model.fit_predict(non_noise_embeddings)\n",
        "\n",
        "                # Map K-Means labels back to original indices, keeping noise points as -1\n",
        "                final_cluster_labels = np.full(len(reduced_embeddings), -1, dtype=int)\n",
        "                for original_idx, kmeans_label in zip(non_noise_indices, kmeans_labels):\n",
        "                    final_cluster_labels[original_idx] = kmeans_label\n",
        "\n",
        "            print(\"K-Means clustering complete (on non-noise points).\")\n",
        "            cluster_labels = final_cluster_labels\n",
        "\n",
        "        else:\n",
        "            print(\"Using OPTICS clustering results.\")\n",
        "            cluster_labels = optics_labels\n",
        "\n",
        "\n",
        "    if cluster_labels is not None:\n",
        "        return {\"cluster_labels\": cluster_labels.tolist()} # Ensure labels are a list for JSON compatibility\n",
        "    else:\n",
        "        return {\"error\": \"Clustering failed to produce labels.\"}\n",
        "\n",
        "# Load a pre-trained KeyBERT model (still useful for keyword suggestions if needed)\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Configure Gemini API\n",
        "try:\n",
        "    # Assuming GOOGLE_API_KEY is already set in the environment or Colab secrets\n",
        "    GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest') # Using a suitable model\n",
        "    print(\"Gemini API configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API: {e}\")\n",
        "    gemini_model = None # Set to None if configuration fails\n",
        "\n",
        "\n",
        "# Define the Cluster Naming Node function\n",
        "def name_clusters(state: GraphState) -> Dict[str, Dict[int, str]]:\n",
        "    \"\"\"\n",
        "    Names the clusters using Gemini API or KeyBERT, extracting keywords from documents within each cluster,\n",
        "    aiming for semantic names and handling potential API failures.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data and cluster_labels.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cluster_names.\n",
        "    \"\"\"\n",
        "    print(\"---CLUSTER NAMING NODE (Refined)---\")\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "    cluster_labels = state.get(\"cluster_labels\") # Use .get() for safer access\n",
        "\n",
        "    if input_data is None or cluster_labels is None:\n",
        "        print(\"Error: Input data or cluster labels are missing for naming.\")\n",
        "        return {\"error\": \"Input data or cluster labels are missing for naming.\"}\n",
        "\n",
        "    # Group documents by cluster label\n",
        "    clustered_docs = defaultdict(list)\n",
        "    for doc, label in zip(input_data, cluster_labels):\n",
        "        clustered_docs[label].append(doc)\n",
        "\n",
        "    cluster_names = {}\n",
        "    # Generate a name for each cluster\n",
        "    for cluster_id, docs in clustered_docs.items():\n",
        "        if cluster_id == -1:\n",
        "            cluster_names[cluster_id] = \"Noise\"\n",
        "            continue\n",
        "\n",
        "        if not docs:\n",
        "            cluster_names[cluster_id] = \"Empty Cluster\"\n",
        "            continue\n",
        "\n",
        "        cluster_name = None # Initialize cluster_name to None\n",
        "\n",
        "        # Use Gemini API for naming if configured\n",
        "        if gemini_model:\n",
        "            print(f\"Attempting to generate name for Cluster {cluster_id} using Gemini API...\")\n",
        "            # Take a sample of documents to avoid exceeding context window\n",
        "            sample_docs = docs[:20] # Use a reasonable sample size\n",
        "            # Refine the prompt to be more direct about the desired output format and constraints\n",
        "            prompt = f\"\"\"Analyze the following texts from a cluster and provide a concise name (maximum 5 words) that summarizes the main topic. Ensure the name is semantic and easy to understand.\n",
        "\n",
        "Texts:\n",
        "{'- '.join(sample_docs)}\n",
        "\n",
        "Concise Name (max 5 words):\"\"\"\n",
        "            try:\n",
        "                response = gemini_model.generate_content(prompt)\n",
        "                if response and response.text:\n",
        "                    cluster_name_raw = response.text.strip()\n",
        "                    # Ensure the concise name is max 5 words\n",
        "                    cluster_name = \" \".join(cluster_name_raw.split()[:5])\n",
        "                    print(f\"Generated name for Cluster {cluster_id} with Gemini API: {cluster_name}\")\n",
        "                else:\n",
        "                    print(f\"Gemini API returned an empty response for Cluster {cluster_id}. Falling back to KeyBERT.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating name for Cluster {cluster_id} with Gemini API: {e}. Falling back to KeyBERT.\")\n",
        "\n",
        "        # Fallback to KeyBERT if Gemini API failed or not configured\n",
        "        if cluster_name is None:\n",
        "            print(f\"Using KeyBERT for Cluster {cluster_id}...\")\n",
        "            cluster_text = \" \".join(docs)\n",
        "            keywords = kw_model.extract_keywords(\n",
        "                cluster_text,\n",
        "                keyphrase_ngram_range=(1, 3),\n",
        "                stop_words='english',\n",
        "                use_mmr=True,\n",
        "                diversity=0.7,\n",
        "                top_n=5\n",
        "            )\n",
        "            keyword_list = [keyword[0] for keyword in keywords]\n",
        "            # Combine keywords into a name, ensuring it's max 5 words\n",
        "            cluster_name = \" \".join(keyword_list).split()[:5]\n",
        "            cluster_name = \" \".join(cluster_name)\n",
        "\n",
        "            print(f\"Generated name for Cluster {cluster_id} with KeyBERT: {cluster_name}\")\n",
        "\n",
        "        cluster_names[cluster_id] = cluster_name\n",
        "\n",
        "\n",
        "    print(\"Cluster naming complete (Refined).\")\n",
        "    return {\"cluster_names\": cluster_names} # Ensure this returns a dictionary to update state\n",
        "\n",
        "# Initialize ChromaDB client (in-memory for this example)\n",
        "client = chromadb.Client()\n",
        "%env CHROMA_ANALYTICS=False\n",
        "\n",
        "# Define the Storage Node function\n",
        "def store_results(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Stores the clustered data and cluster names in ChromaDB.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data, cluster_labels, and cluster_names.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary indicating the storage is complete or an error message.\n",
        "    \"\"\"\n",
        "    print(\"---STORAGE NODE---\")\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "    cluster_labels = state.get(\"cluster_labels\") # Use .get() for safer access\n",
        "    cluster_names = state.get(\"cluster_names\") # Use .get() for safer access\n",
        "\n",
        "    if input_data is None or cluster_labels is None or cluster_names is None:\n",
        "        print(\"Error: Data, labels, or names are missing for storage.\")\n",
        "        return {\"error\": \"Data, labels, or names are missing for storage.\"}\n",
        "\n",
        "    # Create or get a collection\n",
        "    collection_name = \"topic_clusters\"\n",
        "    try:\n",
        "        # Attempt to delete collection if it exists to avoid issues with re-adding\n",
        "        client.delete_collection(name=collection_name)\n",
        "        print(f\"Deleted existing collection: {collection_name}\")\n",
        "    except:\n",
        "        pass # Ignore if collection doesn't exist\n",
        "\n",
        "    try:\n",
        "        collection = client.create_collection(name=collection_name)\n",
        "        print(f\"Created collection: {collection_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating collection: {e}\")\n",
        "        return {\"error\": f\"Error creating collection: {e}\"}\n",
        "\n",
        "\n",
        "    # Prepare data for ChromaDB\n",
        "    ids = [f\"doc_{i}\" for i in range(len(input_data))]\n",
        "    # Store original text and cluster label as metadata\n",
        "    metadatas = []\n",
        "    for i in range(len(input_data)):\n",
        "        metadata = {\"cluster_label\": str(cluster_labels[i])}\n",
        "        # Add cluster name to metadata if available\n",
        "        if cluster_labels[i] in cluster_names:\n",
        "            metadata[\"cluster_name\"] = cluster_names[cluster_labels[i]]\n",
        "        metadatas.append(metadata)\n",
        "\n",
        "\n",
        "    # Add data to the collection\n",
        "    # Note: ChromaDB requires embeddings for add, but we only need to store text and metadata for this task\n",
        "    # A workaround is to use the original embeddings or generate dummy ones if not available.\n",
        "    # For simplicity, we will store the original text as documents and metadata.\n",
        "    # If you need to query by similarity, you would store the embeddings here.\n",
        "    print(f\"Adding {len(input_data)} documents to ChromaDB collection '{collection_name}'...\")\n",
        "    try:\n",
        "        collection.add(\n",
        "            documents=input_data,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "        print(\"Storage complete.\")\n",
        "        return {\"storage_status\": \"complete\"}\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding documents to collection: {e}\")\n",
        "        return {\"error\": f\"Error adding documents to collection: {e}\"}\n",
        "\n",
        "# Define the Visualization Node function\n",
        "def visualize_clusters(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Visualizes the clustered, dimensionality-reduced data using UMAP and cluster labels/names.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with reduced_embeddings, cluster_labels, and cluster_names.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary indicating the visualization is complete or an error message.\n",
        "    \"\"\"\n",
        "    print(\"---VISUALIZATION NODE---\")\n",
        "    reduced_embeddings = state.get(\"reduced_embeddings\") # Use .get() for safer access\n",
        "    cluster_labels = state.get(\"cluster_labels\") # Use .get() for safer access\n",
        "    cluster_names = state.get(\"cluster_names\") # Use .get() for safer access\n",
        "    input_data = state.get(\"input_data\")\n",
        "\n",
        "    if reduced_embeddings is None or cluster_labels is None or cluster_names is None:\n",
        "        print(\"Error: Reduced embeddings, cluster labels, or cluster names are missing for visualization.\")\n",
        "        return {\"error\": \"Reduced embeddings, cluster labels, or cluster names are missing for visualization.\"}\n",
        "\n",
        "    # Ensure reduced_embeddings are in a plottable format (e.g., 2D)\n",
        "    if reduced_embeddings.shape[1] > 2:\n",
        "         print(\"Warning: Reduced embeddings are not 2D. Performing UMAP again for visualization.\")\n",
        "         try:\n",
        "            # Reduce to 2 components specifically for visualization\n",
        "            reducer_2d = umap.UMAP(n_components=2, random_state=42)\n",
        "            reduced_embeddings_2d = reducer_2d.fit_transform(reduced_embeddings)\n",
        "         except Exception as e:\n",
        "             print(f\"Error reducing dimensionality to 2D for visualization: {e}\")\n",
        "             return {\"error\": f\"Error reducing dimensionality to 2D for visualization: {e}\"}\n",
        "    else:\n",
        "        reduced_embeddings_2d = reduced_embeddings\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = sns.scatterplot(\n",
        "        x=reduced_embeddings_2d[:, 0],\n",
        "        y=reduced_embeddings_2d[:, 1],\n",
        "        hue=cluster_labels,\n",
        "        palette='viridis',\n",
        "        legend='full',\n",
        "        alpha=0.7\n",
        "    )\n",
        "\n",
        "    # Add cluster names as labels to the plot (optional, can be crowded)\n",
        "    # You might want to add labels only for cluster centroids or a sample of points\n",
        "    # For simplicity, let's use a legend with names\n",
        "    handles, labels = scatter.get_legend_handles_labels()\n",
        "    # Map numeric labels to cluster names for the legend\n",
        "    named_labels = [cluster_names.get(int(label), f\"Cluster {label}\") for label in labels]\n",
        "    plt.legend(handles, named_labels, title=\"Clusters\")\n",
        "\n",
        "\n",
        "    plt.title('Cluster Visualization (UMAP)')\n",
        "    plt.xlabel('UMAP Component 1')\n",
        "    plt.ylabel('UMAP Component 2')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Visualization complete.\")\n",
        "    return {\"visualization_status\": \"complete\"}\n",
        "\n",
        "\n",
        "# Define the Orchestrator Node function\n",
        "def orchestrator(state: GraphState) -> str:\n",
        "    \"\"\"\n",
        "    Directs the workflow based on the current state and presence of errors.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph.\n",
        "\n",
        "    Returns:\n",
        "        The name of the next node or END.\n",
        "    \"\"\"\n",
        "    print(\"---ORCHESTRATOR NODE---\")\n",
        "    error = state.get(\"error\")\n",
        "    storage_status = state.get(\"storage_status\")\n",
        "    visualization_status = state.get(\"visualization_status\")\n",
        "\n",
        "\n",
        "    # If there's an error, stop the process\n",
        "    if error:\n",
        "        print(f\"Error detected: {error}. Stopping workflow.\")\n",
        "        return END # Indicate end of graph due to error\n",
        "\n",
        "    # Determine next step based on completed steps in sequence\n",
        "    # Check for the latest completed step first\n",
        "    if visualization_status is None and state.get(\"cluster_names\") is not None:\n",
        "         print(\"Visualization not complete. Proceeding to visualization.\")\n",
        "         return \"visualize_clusters\"\n",
        "    elif state.get(\"storage_status\") is None and state.get(\"cluster_names\") is not None:\n",
        "         print(\"Storage not complete. Proceeding to storage.\")\n",
        "         return \"store\"\n",
        "    elif state.get(\"cluster_names\") is None and state.get(\"cluster_labels\") is not None:\n",
        "        print(\"Cluster names not found. Proceeding to cluster naming.\")\n",
        "        return \"name_clusters\"\n",
        "    elif state.get(\"cluster_labels\") is None and state.get(\"reduced_embeddings\") is not None:\n",
        "        print(\"Cluster labels not found. Proceeding to clustering.\")\n",
        "        return \"cluster\"\n",
        "    elif state.get(\"reduced_embeddings\") is None and state.get(\"embeddings\") is not None:\n",
        "        print(\"Reduced embeddings not found. Proceeding to dimensionality reduction.\")\n",
        "        return \"reduce_dim\"\n",
        "    elif state.get(\"embeddings\") is None and state.get(\"cleaned_data\") is not None:\n",
        "        print(\"Embeddings not found. Proceeding to vector extraction.\")\n",
        "        return \"embed\"\n",
        "    elif state.get(\"cleaned_data\") is None:\n",
        "        print(\"Cleaned data not found. Proceeding to data cleaning.\")\n",
        "        return \"clean\"\n",
        "    else:\n",
        "        print(\"All processing steps complete. Ending workflow.\")\n",
        "        return END\n",
        "\n",
        "\n",
        "# Define the LangGraph workflow\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add nodes for each stage\n",
        "workflow.add_node(\"clean\", clean_data)\n",
        "workflow.add_node(\"embed\", extract_embeddings)\n",
        "workflow.add_node(\"reduce_dim\", reduce_dimensionality)\n",
        "workflow.add_node(\"cluster\", cluster_data)\n",
        "workflow.add_node(\"name_clusters\", name_clusters)\n",
        "workflow.add_node(\"store\", store_results)\n",
        "workflow.add_node(\"visualize_clusters\", visualize_clusters) # Add visualization node\n",
        "workflow.add_node(\"orchestrator\", orchestrator) # Add the orchestrator node\n",
        "\n",
        "\n",
        "# Set the entry point\n",
        "workflow.set_entry_point(\"orchestrator\")\n",
        "\n",
        "# Define the edges (transitions) between nodes\n",
        "# Each node transitions back to the orchestrator to decide the next step\n",
        "workflow.add_edge(\"clean\", \"orchestrator\")\n",
        "workflow.add_edge(\"embed\", \"orchestrator\")\n",
        "workflow.add_edge(\"reduce_dim\", \"orchestrator\")\n",
        "workflow.add_edge(\"cluster\", \"orchestrator\")\n",
        "workflow.add_edge(\"name_clusters\", \"orchestrator\")\n",
        "workflow.add_edge(\"visualize_clusters\", \"orchestrator\") # Add edge from visualize to orchestrator\n",
        "workflow.add_edge(\"store\", \"orchestrator\") # After storage, go back to orchestrator to potentially end\n",
        "\n",
        "# Add conditional edges from the orchestrator\n",
        "# The orchestrator's return value (the string name of the next node or END)\n",
        "# will determine which node to execute next.\n",
        "workflow.add_conditional_edges(\n",
        "    \"orchestrator\",\n",
        "    orchestrator, # The orchestrator function directly returns the next node name or END\n",
        "    {\n",
        "        \"clean\": \"clean\",\n",
        "        \"embed\": \"embed\",\n",
        "        \"reduce_dim\": \"reduce_dim\",\n",
        "        \"cluster\": \"cluster\",\n",
        "        \"name_clusters\": \"name_clusters\",\n",
        "        \"visualize_clusters\": \"visualize_clusters\", # Add visualization transition\n",
        "        \"store\": \"store\",\n",
        "        END: END # If orchestrator returns END, the workflow stops\n",
        "    }\n",
        ")\n",
        "\n",
        "# Compile the workflow\n",
        "app = workflow.compile()\n",
        "\n",
        "# Run the workflow with the sample data, setting num_clusters to 2 as requested\n",
        "inputs = {\"input_data\": sample_data, \"num_clusters\": 2}\n",
        "final_state = app.invoke(inputs)\n",
        "\n",
        "print(\"\\n---Workflow Execution Complete---\")\n",
        "# You can now access the results in the final_state variable\n",
        "# For example:\n",
        "# print(final_state['cluster_names'])\n",
        "# print(final_state['cluster_labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6Hk-ODGCkZE"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to research alternative sentence embedding models suitable for distinguishing humor from non-humor, select the most promising one, update the `extract_embeddings` function to use the new model, and replace the old 'embed' node in the workflow. I will start by researching models and then implement the change.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-kO87WaPCttG"
      },
      "outputs": [],
      "source": [
        "# Research alternative sentence embedding models\n",
        "\n",
        "# Based on research, models trained on sentiment or nuanced language understanding might be better.\n",
        "# Options to consider:\n",
        "# 1. all-MiniLM-L6-v2: Good general-purpose model, faster and smaller than DistilBERT.\n",
        "# 2. roberta-large-nli-stsb-mean-tokens: Larger model, potentially better performance on nuanced text.\n",
        "# 3. tweetnlp/TweetNLP-Sentence-Embedding-base: Trained on tweets, might capture informal language/humor better.\n",
        "# 4. average_word_embeddings_glove.6B.300d: Simpler approach, might be less effective but fast.\n",
        "\n",
        "# Considering the need to distinguish humor and potentially informal language,\n",
        "# 'tweetnlp/TweetNLP-Sentence-Embedding-base' seems like a promising option.\n",
        "# It's trained on tweets which often contain humor and informal language.\n",
        "\n",
        "# Install the necessary library for the new model if not already installed\n",
        "# %pip install tweetnlp --quiet # tweetnlp is not in standard SentenceTransformers\n",
        "\n",
        "# Update the extract_embeddings function\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load the new pre-trained sentence transformer model\n",
        "# Using 'tweetnlp/TweetNLP-Sentence-Embedding-base'\n",
        "try:\n",
        "    new_model = SentenceTransformer('tweetnlp/TweetNLP-Sentence-Embedding-base')\n",
        "    print(\"Loaded new embedding model: tweetnlp/TweetNLP-Sentence-Embedding-base\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading new embedding model: {e}. Falling back to 'all-MiniLM-L6-v2'\")\n",
        "    # Fallback to a reliable general-purpose model if the preferred one fails\n",
        "    new_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(\"Loaded fallback embedding model: all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "# Define the updated Vector Extraction Node function\n",
        "def extract_embeddings(state: GraphState) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extracts vector embeddings from cleaned text data using the selected pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with cleaned_data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with embeddings.\n",
        "    \"\"\"\n",
        "    print(\"---VECTOR EXTRACTION NODE (Updated)---\")\n",
        "    cleaned_data = state[\"cleaned_data\"]\n",
        "\n",
        "    if cleaned_data is None:\n",
        "        print(\"Error: No cleaned data available for embedding.\")\n",
        "        return {\"error\": \"No cleaned data available for embedding.\"}\n",
        "\n",
        "    print(f\"Extracting embeddings for {len(cleaned_data)} texts using the updated model...\")\n",
        "    # Generate embeddings using the new model\n",
        "    embeddings = new_model.encode(cleaned_data)\n",
        "    print(\"Embeddings extraction complete (Updated).\")\n",
        "    return {\"embeddings\": embeddings}\n",
        "\n",
        "# Replace the existing 'embed' node in the workflow with the updated function\n",
        "# Remove the old node before adding the new one\n",
        "nodes_to_remove = [name for name, node in workflow.nodes.items() if node == workflow.nodes.get(\"embed\")]\n",
        "for name in nodes_to_remove:\n",
        "    workflow.nodes.pop(name)\n",
        "\n",
        "workflow.add_node(\"embed\", extract_embeddings)\n",
        "\n",
        "print(\"Vector Extraction Node updated and added to the workflow.\")\n",
        "\n",
        "# Re-compile the workflow with the updated node\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled with updated embedding node.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "661ac057"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "\n",
        "# Define the Dimensionality Reduction Node function\n",
        "def reduce_dimensionality(state: GraphState) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Reduces the dimensionality of vector embeddings using UMAP.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with reduced_embeddings.\n",
        "    \"\"\"\n",
        "    print(\"---DIMENSIONALITY REDUCTION NODE---\")\n",
        "    embeddings = state[\"embeddings\"]\n",
        "    input_data = state[\"input_data\"] # Need input_data to determine dimensionality\n",
        "\n",
        "    if embeddings is None:\n",
        "        print(\"Error: No embeddings available for dimensionality reduction.\")\n",
        "        return {\"error\": \"No embeddings available for dimensionality reduction.\"}\n",
        "\n",
        "    n_samples = len(input_data)\n",
        "    # Determine target dimensionality based on the number of samples\n",
        "    if n_samples <= 500:\n",
        "        n_components = 20\n",
        "    elif n_samples <= 5000:\n",
        "        n_components = 30\n",
        "    elif n_samples <= 20000:\n",
        "        n_components = 50\n",
        "    else:\n",
        "        n_components = 100\n",
        "\n",
        "    print(f\"Reducing dimensionality to {n_components} using UMAP...\")\n",
        "    # Initialize and fit UMAP\n",
        "    reducer = umap.UMAP(n_components=n_components, random_state=42)\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "    print(\"Dimensionality reduction complete.\")\n",
        "    return {\"reduced_embeddings\": reduced_embeddings}\n",
        "\n",
        "# Now, replace the placeholder node in the workflow with the actual function\n",
        "# Remove the old placeholder node before adding the new one if it exists\n",
        "nodes_to_remove = [name for name, node in workflow.nodes.items() if node == workflow.nodes.get(\"reduce_dim\")]\n",
        "for name in nodes_to_remove:\n",
        "    workflow.nodes.pop(name)\n",
        "\n",
        "workflow.add_node(\"reduce_dim\", reduce_dimensionality)\n",
        "\n",
        "print(\"Dimensionality Reduction Node implemented and added to the workflow.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm-IosEdES4K"
      },
      "source": [
        "**Reasoning**:\n",
        "I will modify the `cluster_data` function to exclusively use KMeans with `n_clusters=2` on the `reduced_embeddings` when `num_clusters` is 2, as instructed. This will guarantee exactly two clusters with no noise points in this specific scenario. I will then replace the existing 'cluster' node in the workflow with this modified function and re-compile the workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b1267a74"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Define the Clustering Node function\n",
        "def cluster_data(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Clusters the dimensionality-reduced data using K-Means with n_clusters=2\n",
        "    when num_clusters is set to 2 in the state, ensuring no noise points.\n",
        "    Retains OPTICS logic for other num_clusters values or when num_clusters is not provided.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with reduced_embeddings and optional num_clusters.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cluster_labels or an error message.\n",
        "    \"\"\"\n",
        "    print(\"---CLUSTERING NODE---\")\n",
        "    reduced_embeddings = state[\"reduced_embeddings\"]\n",
        "    num_clusters = state.get(\"num_clusters\")\n",
        "\n",
        "    if reduced_embeddings is None:\n",
        "        print(\"Error: No reduced embeddings available for clustering.\")\n",
        "        return {\"error\": \"No reduced embeddings available for clustering.\"}\n",
        "\n",
        "    cluster_labels = None\n",
        "\n",
        "    # If num_clusters is specifically 2, use K-Means on all data points\n",
        "    if num_clusters == 2:\n",
        "        print(f\"Applying K-Means to achieve exactly {num_clusters} clusters on all data points...\")\n",
        "        try:\n",
        "            kmeans_model = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "            cluster_labels = kmeans_model.fit_predict(reduced_embeddings)\n",
        "            print(\"K-Means clustering complete (2 clusters, no noise).\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during K-Means clustering: {e}\")\n",
        "            return {\"error\": f\"K-Means clustering failed: {e}\"}\n",
        "\n",
        "    # Otherwise, use OPTICS or K-Means on non-noise points if num_clusters is specified and not 2\n",
        "    else:\n",
        "        print(\"Performing clustering using OPTICS...\")\n",
        "        # Use OPTICS to find clusters and identify noise points\n",
        "        optics_model = OPTICS(min_samples=10, xi=0.05, min_cluster_size=0.05)\n",
        "        optics_model.fit(reduced_embeddings)\n",
        "\n",
        "        optics_labels = optics_model.labels_\n",
        "        noise_points = optics_labels == -1\n",
        "        n_noise = list(optics_labels).count(-1)\n",
        "\n",
        "        print(f\"OPTICS found {len(set(optics_labels)) - (1 if -1 in optics_labels else 0)} clusters and {n_noise} noise points.\")\n",
        "\n",
        "        if num_clusters is not None and num_clusters > 0:\n",
        "            print(f\"Applying K-Means to achieve {num_clusters} clusters on non-noise points...\")\n",
        "            # Filter out noise points for K-Means\n",
        "            non_noise_indices = np.where(~noise_points)[0]\n",
        "            non_noise_embeddings = reduced_embeddings[non_noise_indices]\n",
        "\n",
        "            if len(non_noise_embeddings) == 0:\n",
        "                print(\"Warning: No non-noise points to apply K-Means.\")\n",
        "                # Assign -1 to all points if no non-noise points\n",
        "                final_cluster_labels = np.full(len(reduced_embeddings), -1, dtype=int)\n",
        "            elif num_clusters > len(non_noise_embeddings):\n",
        "                 print(f\"Warning: Requested number of clusters ({num_clusters}) is greater than the number of non-noise points ({len(non_noise_embeddings)}). Using OPTICS labels.\")\n",
        "                 final_cluster_labels = optics_labels\n",
        "            else:\n",
        "                # Apply K-Means\n",
        "                kmeans_model = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "                kmeans_labels = kmeans_model.fit_predict(non_noise_embeddings)\n",
        "\n",
        "                # Map K-Means labels back to original indices, keeping noise points as -1\n",
        "                final_cluster_labels = np.full(len(reduced_embeddings), -1, dtype=int)\n",
        "                for original_idx, kmeans_label in zip(non_noise_indices, kmeans_labels):\n",
        "                    final_cluster_labels[original_idx] = kmeans_label\n",
        "\n",
        "            print(\"K-Means clustering complete (on non-noise points).\")\n",
        "            cluster_labels = final_cluster_labels\n",
        "\n",
        "        else:\n",
        "            print(\"Using OPTICS clustering results.\")\n",
        "            cluster_labels = optics_labels\n",
        "\n",
        "\n",
        "    if cluster_labels is not None:\n",
        "        return {\"cluster_labels\": cluster_labels.tolist()}\n",
        "    else:\n",
        "        return {\"error\": \"Clustering failed to produce labels.\"}\n",
        "\n",
        "\n",
        "# Now, replace the placeholder node in the workflow with the actual function\n",
        "# Remove the old placeholder node before adding the new one if it exists\n",
        "nodes_to_remove = [name for name, node in workflow.nodes.items() if node == workflow.nodes.get(\"cluster\")]\n",
        "for name in nodes_to_remove:\n",
        "    workflow.nodes.pop(name)\n",
        "\n",
        "workflow.add_node(\"cluster\", cluster_data)\n",
        "\n",
        "print(\"Clustering Node modified and updated in the workflow to force 2 clusters with KMeans when num_clusters=2.\")\n",
        "\n",
        "# Re-compile the workflow with the updated node\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled with updated clustering node.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N27PNplOHB70"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "from collections import defaultdict\n",
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "\n",
        "# Load a pre-trained KeyBERT model (still useful for keyword suggestions if needed)\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Configure Gemini API\n",
        "try:\n",
        "    # Assuming GOOGLE_API_KEY is already set in the environment or Colab secrets\n",
        "    genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "    gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest') # Using a suitable model\n",
        "    print(\"Gemini API configured successfully for cluster naming.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API for cluster naming: {e}\")\n",
        "    gemini_model = None # Set to None if configuration fails\n",
        "\n",
        "\n",
        "# Define the Cluster Naming Node function\n",
        "def name_clusters(state: GraphState) -> Dict[str, Dict[int, str]]:\n",
        "    \"\"\"\n",
        "    Names the clusters, specifically aiming for \"Humor\" and \"Non-Humor\" using Gemini API\n",
        "    (preferred) or inferring from KeyBERT keywords, ensuring names are maximum 5 words.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data and cluster_labels.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cluster_names.\n",
        "    \"\"\"\n",
        "    print(\"---CLUSTER NAMING NODE (Refined)---\")\n",
        "    input_data = state[\"input_data\"]\n",
        "    cluster_labels = state[\"cluster_labels\"]\n",
        "\n",
        "    if input_data is None or cluster_labels is None:\n",
        "        print(\"Error: Input data or cluster labels are missing for naming.\")\n",
        "        return {\"error\": \"Input data or cluster labels are missing for naming.\"}\n",
        "\n",
        "    # Group documents by cluster label\n",
        "    clustered_docs = defaultdict(list)\n",
        "    for doc, label in zip(input_data, cluster_labels):\n",
        "        clustered_docs[label].append(doc)\n",
        "\n",
        "    cluster_names = {}\n",
        "    # Generate a name for each cluster\n",
        "    for cluster_id, docs in clustered_docs.items():\n",
        "        if cluster_id == -1:\n",
        "            cluster_names[cluster_id] = \"Noise\"\n",
        "            continue\n",
        "\n",
        "        if not docs:\n",
        "            cluster_names[cluster_id] = \"Empty Cluster\"\n",
        "            continue\n",
        "\n",
        "        cluster_name = None # Initialize cluster_name to None\n",
        "\n",
        "        # Use Gemini API for naming if configured\n",
        "        if gemini_model:\n",
        "            print(f\"Attempting to generate name for Cluster {cluster_id} using Gemini API...\")\n",
        "            # Take a sample of documents to avoid exceeding context window\n",
        "            sample_docs = docs[:20] # Use the increased sample size\n",
        "            # Refine the prompt to be more direct about the desired categories\n",
        "            prompt = f\"\"\"Analyze the following texts from a cluster and determine if the main topic is \"Humor\" or \"Non-Humor\". Respond with ONLY one of the following: \"Humor\", \"Non-Humor\", or a concise name (maximum 5 words) if neither category fits well.\n",
        "\n",
        "Texts:\n",
        "{'- '.join(sample_docs)}\n",
        "\n",
        "Category or Concise Name (max 5 words):\"\"\"\n",
        "            try:\n",
        "                response = gemini_model.generate_content(prompt)\n",
        "                if response and response.text:\n",
        "                    cluster_name_raw = response.text.strip()\n",
        "                    # Check if the response is one of the specific categories\n",
        "                    if cluster_name_raw.lower() in [\"humor\", \"non-humor\"]:\n",
        "                        cluster_name = cluster_name_raw\n",
        "                    else:\n",
        "                        # Otherwise, ensure the concise name is max 5 words\n",
        "                        cluster_name = \" \".join(cluster_name_raw.split()[:5])\n",
        "\n",
        "                    print(f\"Generated name for Cluster {cluster_id} with Gemini API: {cluster_name}\")\n",
        "                else:\n",
        "                    print(f\"Gemini API returned an empty response for Cluster {cluster_id}. Falling back to KeyBERT.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating name for Cluster {cluster_id} with Gemini API: {e}. Falling back to KeyBERT.\")\n",
        "\n",
        "        # Fallback to KeyBERT if Gemini API failed or not configured\n",
        "        if cluster_name is None:\n",
        "            print(f\"Using KeyBERT for Cluster {cluster_id}...\")\n",
        "            cluster_text = \" \".join(docs)\n",
        "            keywords = kw_model.extract_keywords(\n",
        "                cluster_text,\n",
        "                keyphrase_ngram_range=(1, 3),\n",
        "                stop_words='english',\n",
        "                use_mmr=True,\n",
        "                diversity=0.7,\n",
        "                top_n=5\n",
        "            )\n",
        "            keyword_list = [keyword[0] for keyword in keywords]\n",
        "            # Attempt to infer \"Humor\" or \"Non-Humor\" from keywords\n",
        "            inferred_name_check = \" \".join(keyword_list).lower()\n",
        "            if any(word in inferred_name_check for word in [\"joke\", \"humor\", \"funny\", \"laugh\", \"quirky\", \"comedy\"]):\n",
        "                cluster_name = \"Humor (Inferred)\"\n",
        "            elif any(word in inferred_name_check for word in [\"politics\", \"news\", \"study\", \"report\", \"health\", \"business\", \"technology\", \"science\", \"world\", \"government\", \"election\"]):\n",
        "                 cluster_name = \"Non-Humor (Inferred)\"\n",
        "            else:\n",
        "                 cluster_name = \" \".join(keyword_list).split()[:5]\n",
        "                 cluster_name = \" \".join(cluster_name)\n",
        "\n",
        "            print(f\"Generated name for Cluster {cluster_id} with KeyBERT: {cluster_name}\")\n",
        "\n",
        "        cluster_names[cluster_id] = cluster_name\n",
        "\n",
        "\n",
        "    print(\"Cluster naming complete (Refined).\")\n",
        "    return {\"cluster_names\": cluster_names}\n",
        "\n",
        "# Now, replace the placeholder node in the workflow with the actual function\n",
        "# Remove the old placeholder node before adding the new one if it exists\n",
        "nodes_to_remove = [name for name, node in workflow.nodes.items() if node == workflow.nodes.get(\"name_clusters\")]\n",
        "for name in nodes_to_remove:\n",
        "    workflow.nodes.pop(name)\n",
        "\n",
        "\n",
        "workflow.add_node(\"name_clusters\", name_clusters)\n",
        "\n",
        "print(\"Cluster Naming Node refined and updated in the workflow.\")\n",
        "\n",
        "# Re-compile the workflow with the updated nodes\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow re-compiled successfully.\")\n",
        "\n",
        "# Sample data for testing (assuming sample_data is still defined)\n",
        "# Define the initial state with the sample data and num_clusters set to 2\n",
        "initial_state = {\"input_data\": sample_data, \"num_clusters\": 2} # Set num_clusters to 2\n",
        "\n",
        "# Run the LangGraph application\n",
        "print(\"\\nRunning the LangGraph application with num_clusters=2...\")\n",
        "# The .invoke() method runs the graph once\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "print(\"\\n---FINAL STATE (with num_clusters=2)---\")\n",
        "# Display the final state of the graph\n",
        "print(final_state)\n",
        "\n",
        "# Print the generated cluster names\n",
        "print(\"\\nGenerated Cluster Names:\", final_state.get(\"cluster_names\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a68fd64b"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "# Initialize ChromaDB client (in-memory for this example)\n",
        "client = chromadb.Client()\n",
        "%env CHROMA_ANALYTICS=False\n",
        "\n",
        "# Define the Storage Node function\n",
        "def store_results(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Stores the clustered data and cluster names in ChromaDB.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data, cluster_labels, and cluster_names.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary indicating the storage is complete or an error message.\n",
        "    \"\"\"\n",
        "    print(\"---STORAGE NODE---\")\n",
        "    input_data = state[\"input_data\"]\n",
        "    cluster_labels = state[\"cluster_labels\"]\n",
        "    cluster_names = state[\"cluster_names\"]\n",
        "\n",
        "    if input_data is None or cluster_labels is None or cluster_names is None:\n",
        "        print(\"Error: Data, labels, or names are missing for storage.\")\n",
        "        return {\"error\": \"Data, labels, or names are missing for storage.\"}\n",
        "\n",
        "    # Create or get a collection\n",
        "    collection_name = \"topic_clusters\"\n",
        "    try:\n",
        "        collection = client.create_collection(name=collection_name)\n",
        "        print(f\"Created collection: {collection_name}\")\n",
        "    except: # Handle case where collection already exists\n",
        "        collection = client.get_collection(name=collection_name)\n",
        "        print(f\"Using existing collection: {collection_name}\")\n",
        "\n",
        "\n",
        "    # Prepare data for ChromaDB\n",
        "    ids = [f\"doc_{i}\" for i in range(len(input_data))]\n",
        "    # Store original text and cluster label as metadata\n",
        "    metadatas = []\n",
        "    for i in range(len(input_data)):\n",
        "        metadata = {\"cluster_label\": str(cluster_labels[i])}\n",
        "        # Add cluster name to metadata if available\n",
        "        if cluster_labels[i] in cluster_names:\n",
        "            metadata[\"cluster_name\"] = cluster_names[cluster_labels[i]]\n",
        "        metadatas.append(metadata)\n",
        "\n",
        "\n",
        "    # Add data to the collection\n",
        "    # Note: ChromaDB requires embeddings for add, but we only need to store text and metadata for this task\n",
        "    # A workaround is to use the original embeddings or generate dummy ones if not available.\n",
        "    # For simplicity, we will store the original text as documents and metadata.\n",
        "    # If you need to query by similarity, you would store the embeddings here.\n",
        "    print(f\"Adding {len(input_data)} documents to ChromaDB collection '{collection_name}'...\")\n",
        "    collection.add(\n",
        "        documents=input_data,\n",
        "        metadatas=metadatas,\n",
        "        ids=ids\n",
        "    )\n",
        "\n",
        "    print(\"Storage complete.\")\n",
        "    return {\"storage_status\": \"complete\"}\n",
        "\n",
        "# Now, replace the placeholder node in the workflow with the actual function\n",
        "# Remove the old placeholder node before adding the new one if it exists\n",
        "nodes_to_remove = [name for name, node in workflow.nodes.items() if node == workflow.nodes.get(\"store\")]\n",
        "for name in nodes_to_remove:\n",
        "    workflow.nodes.pop(name)\n",
        "\n",
        "workflow.add_node(\"store\", store_results)\n",
        "\n",
        "print(\"Storage Node implemented and added to the workflow.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c059dc03"
      },
      "source": [
        "## Analyze and present results\n",
        "\n",
        "### Subtask:\n",
        "Examine the new clustering results (labels and noise) and the generated cluster names. Present the updated visualization and document lists per cluster to the user for review.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a61ab10"
      },
      "source": [
        "**Reasoning**:\n",
        "I will generate a 2D UMAP visualization of the reduced embeddings, coloring points by cluster labels and including a legend with cluster names. Then I will iterate through each unique cluster, print its ID and name, and list the documents belonging to that cluster.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "538f0a1e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Assuming 'final_state' contains the results after running the workflow\n",
        "# Make sure to run the workflow execution cell (e.g., cell ID 3ad91cd3) first\n",
        "if 'final_state' in locals():\n",
        "    reduced_embeddings = final_state.get(\"reduced_embeddings\")\n",
        "    cluster_labels = final_state.get(\"cluster_labels\")\n",
        "    cluster_names = final_state.get(\"cluster_names\")\n",
        "    input_data = final_state.get(\"input_data\")\n",
        "\n",
        "\n",
        "    if reduced_embeddings is not None and cluster_labels is not None:\n",
        "        print(\"Generating 2D visualization of clusters...\")\n",
        "\n",
        "        # Reduce dimensionality to 2 components specifically for visualization\n",
        "        # Use a consistent random_state for reproducibility\n",
        "        umap_visualizer = umap.UMAP(n_components=2, random_state=42)\n",
        "        # Need to handle cases where reduced_embeddings might be empty or not suitable\n",
        "        try:\n",
        "            reduced_for_viz = umap_visualizer.fit_transform(reduced_embeddings)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during UMAP reduction for visualization: {e}\")\n",
        "            print(\"Could not generate visualization.\")\n",
        "            reduced_for_viz = None\n",
        "\n",
        "\n",
        "        if reduced_for_viz is not None:\n",
        "            # Create the scatter plot\n",
        "            plt.figure(figsize=(10, 8))\n",
        "\n",
        "            # Get unique cluster labels (excluding noise if present)\n",
        "            unique_labels = sorted(list(set(cluster_labels)))\n",
        "\n",
        "            # Assign a color to each cluster - ensure enough distinct colors for 2 clusters + noise\n",
        "            # Using a colormap that provides distinct colors for a small number of categories\n",
        "            colors = plt.cm.get_cmap('tab10', len(unique_labels))\n",
        "\n",
        "\n",
        "            for i, label in enumerate(unique_labels):\n",
        "                if label == -1:\n",
        "                    # Plot noise points in black\n",
        "                    color = 'black'\n",
        "                    label_name = cluster_names.get(label, \"Noise\")\n",
        "                    alpha = 0.5\n",
        "                    marker = 'x'\n",
        "                else:\n",
        "                    # Plot regular clusters with colors\n",
        "                    color = colors(i)\n",
        "                    label_name = cluster_names.get(label, f\"Cluster {label}\")\n",
        "                    alpha = 0.8\n",
        "                    marker = 'o'\n",
        "\n",
        "                # Select points belonging to the current cluster\n",
        "                clustered_points = reduced_for_viz[np.array(cluster_labels) == label]\n",
        "\n",
        "                # Plot the points\n",
        "                plt.scatter(\n",
        "                    clustered_points[:, 0],\n",
        "                    clustered_points[:, 1],\n",
        "                    s=10, # Size of points\n",
        "                    color=color,\n",
        "                    label=label_name,\n",
        "                    alpha=alpha,\n",
        "                    marker=marker\n",
        "                )\n",
        "\n",
        "            plt.title('Topic Clustering Visualization (UMAP 2D)')\n",
        "            plt.xlabel('UMAP Component 1')\n",
        "            plt.ylabel('UMAP Component 2')\n",
        "            plt.legend(title='Clusters', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "            plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"Reduced embeddings or cluster labels not found in final_state. Please run the workflow first.\")\n",
        "\n",
        "    # Now, print the documents per cluster\n",
        "    if input_data is not None and cluster_labels is not None:\n",
        "        print(\"\\n--- DOCUMENTS PER CLUSTER ---\")\n",
        "\n",
        "        # Group documents by cluster label\n",
        "        clustered_docs = defaultdict(list)\n",
        "        for doc, label in zip(input_data, cluster_labels):\n",
        "            clustered_docs[label].append(doc)\n",
        "\n",
        "        # Display documents for each cluster\n",
        "        # Sort cluster IDs for consistent output, excluding -1 if it exists and is not needed\n",
        "        sorted_cluster_ids = sorted([label for label in clustered_docs.keys() if label != -1])\n",
        "        if -1 in clustered_docs: # Include noise if present\n",
        "            sorted_cluster_ids.append(-1)\n",
        "\n",
        "\n",
        "        for cluster_id in sorted_cluster_ids:\n",
        "            docs = clustered_docs[cluster_id]\n",
        "            cluster_name = cluster_names.get(cluster_id, f\"Cluster {cluster_id}\" if cluster_id != -1 else \"Noise\")\n",
        "\n",
        "            print(f\"\\n--- Cluster {cluster_id} ({cluster_name}) ---\")\n",
        "            if not docs:\n",
        "                print(\"  No documents in this cluster.\")\n",
        "            else:\n",
        "                for i, doc in enumerate(docs):\n",
        "                    print(f\"  {i+1}. {doc}\")\n",
        "\n",
        "    else:\n",
        "        print(\"Input data or cluster labels not found in final_state. Please run the workflow first.\")\n",
        "\n",
        "else:\n",
        "    print(\"final_state variable not found. Please run the workflow execution cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uGShvCbGHkze"
      },
      "outputs": [],
      "source": [
        "# Sample data for testing\n",
        "sample_data = [\n",
        "\"Joe biden rules out 2020 bid: 'guys, i'm not running'\",\n",
        "\"Watch: darvish gave hitter whiplash with slow pitch\",\n",
        "\"What do you call a turtle without its shell? dead.\",\n",
        "\"5 reasons the 2016 election feels so personal\",\n",
        "\"Pasco police shot mexican migrant from behind, new autopsy shows\",\n",
        "\"Martha stewart tweets hideous food photo, twitter responds accordingly\",\n",
        "\"What is a pokemon master's favorite kind of pasta? wartortellini!\",\n",
        "\"Why do native americans hate it when it rains in april? because it brings mayflowers.\",\n",
        "\"Obama's climate change legacy is impressive, imperfect and vulnerable\",\n",
        "\"My family tree is a cactus, we're all pricks.\",\n",
        "\"Donald trump has found something mysterious for rudy giuliani to do\",\n",
        "\"How donald trump and ted cruz's love affair is all relationships\",\n",
        "\"Want to know why athletes chose to #takeaknee? look at our broken justice system\",\n",
        "\"How are music and candy similar? we throw away the rappers.\",\n",
        "\"Famous couples who help each other stay healthy and fit\",\n",
        "\"Study finds strong link between zika and guillain-barre syndrome\",\n",
        "\"Alec baldwin and wife hilaria welcome another baby boy\",\n",
        "\"Trump says iran is complying with nuclear deal, but remains a dangerous threat\",\n",
        "\"Kim kardashian baby name: reality star discusses the 'k' name possibility (video)\",\n",
        "\"I just ended a 5 year relationship i'm fine, it wasn't my relationship\",\n",
        "\"Here's what the oscar nominations should look like\",\n",
        "\"Dating tip: surprise your date! show up a day early.\",\n",
        "\"Reflections from davos: leaders deliberate what's next for climate action after paris deal\",\n",
        "\"What do you call an explanation of an asian cooking show? a wok-through.\",\n",
        "\"Swimming toward a brighter future: how i was introduced to the world of autism\",\n",
        "\"Why did little miss muffet have gps on her tuffet? to keep her from losing her whey.\",\n",
        "\"The pixelated 'simpsons' should be a real couch gag\",\n",
        "\"All pants are breakaway pants if you're angry enough\",\n",
        "\"Watch: former british open champ makes embarrassing putting fail\",\n",
        "\"Chrissy teigen's 2015 grammy dress is skintight and perfect\"\n",
        "\"Ugh, I just spilled red wine all over the inside of my tummy.\",\n",
        "\"The next iPhone update will help you save lives.\",\n",
        "\"Celebrating the fourth of July with airport profiling.\",\n",
        "\"The Big Bend, a U-shaped skyscraper, could become the longest in the world.\",\n",
        "\"Oscars 2016 red carpet: all the stunning looks from the Academy Awards.\",\n",
        "\"Why do Jews have big noses? Because the air is free.\",\n",
        "\"Interesting fact: by the year 2020 all actors on American TV shows will be Australian.\",\n",
        "\"I'd tell you a chemistry joke but I know I won't get a reaction.\",\n",
        "\"Arkansas approves law to let people carry guns in bars and at public colleges.\",\n",
        "\"On set with Paul Mitchell: from our network.\",\n",
        "\"Did you know diarrhea is genetic? It runs in your jeans.\",\n",
        "\"My son's Ebola joke: What do Africans have for breakfast? Ebola cereal :) (Be kind, he's only 14 lol).\",\n",
        "\"What was the sci-fi remake of A Streetcar Named Desire? Interstelllllllaaaaaaar.\",\n",
        "\"What do you call a clan of barbarians you can't see? Invisigoths.\",\n",
        "\"How do you know if someone is using recursion?\",\n",
        "\"Why shouldn't you change around a Pokémon? Because he might peek at chu.\",\n",
        "\"Stolen moment of the week: Andy Ofiesh and Kaytlin Bailey at The Creek and The Cave.\",\n",
        "\"Obama welcomes Pope Francis to the White House.\",\n",
        "\"What do chicken families do on Saturday afternoon? They go on peck-nics!\",\n",
        "\"Hiring a cleaning company: A how-to for everyone who wants to go green.\",\n",
        "\"Explore America’s stunning marine sanctuaries without getting wet.\",\n",
        "\"Do you show up in life in all your amazing glory?\",\n",
        "\"What do JCPenney and teenagers have in common? Pants 50% off.\",\n",
        "\"Has a conversation in my head - cackles with mirth.\",\n",
        "\"Valentine's dinner stress: 4 things not to worry about.\",\n",
        "\"Broadway stars join forces to fight North Carolina's anti-LGBT law.\",\n",
        "\"I'm really sick of making my dog a birthday cake every 52 days.\",\n",
        "\"Knock knock. Who's there? Cotton! Cotton who? Cotton a trap!\",\n",
        "\"Safer driving at the flick of a switch.\",\n",
        "\"Trump refuses to blame himself for GOP 'not getting the job done'.\",\n",
        "\"What do you call a black guy who's hitch-hiking? Stranded!\",\n",
        "\"LeBron James doesn't totally deny the possibility of starring in 'Space Jam 2'.\",\n",
        "\"Why do they say all minorities look the same? Because once you've seen Juan, you've seen Jamaul.\",\n",
        "\"Eve Ensler wants to topple the patriarchy with 'revolutionary love'.\",\n",
        "\"Yo momma so ugly ... her portraits hang themselves.\",\n",
        "\"Kanye West is opening 21 Pablo pop-up shops this weekend.\",\n",
        "\"How to turn leftover champagne into fancy vinegar.\",\n",
        "\"Mesmerizing time-lapse video shows dad making amazing Disney bedroom.\",\n",
        "\"Wife: You forgot to run the dishwasher again, didn't you? Me: No, why?\",\n",
        "\"Resume design: Eye-tracking study finds job seekers have six seconds to make an impression (video).\",\n",
        "\"My friend is dealing with a really severe Viagra addiction. He's having a hard time with it.\",\n",
        "\"Phil Collins cancels comeback shows after being rushed to hospital.\",\n",
        "\"I nicknamed my girlfriend Melody because it's loudest and always on top.\",\n",
        "\"121 members of Congress call for end of restrictions on gun violence research.\",\n",
        "\"How's my life? Let's just say I'm starting a lot of sentences with 'Let's just say.'\",\n",
        "\"Be who you are, no matter what anyone else thinks.\",\n",
        "\"Just imagine how good prescription cheese would be.\",\n",
        "\"Crazy ex-girlfriends are like a box of chocolates — they will kill your dog.\",\n",
        "\"The deeper reason Trump's taco tweet is offensive.\",\n",
        "\"Britney Spears is just a regular mom who loves embarrassing her kids.\",\n",
        "\"Did you hear about the flying German pancakes in WWII? They were in the Luftwaffle!\",\n",
        "\"Kellyanne Conway is wearing a $3,600 coat studded with (ahem) cats.\",\n",
        "\"Steelers coach incensed by headset situation at Gillette Stadium.\",\n",
        "\"The flame of beauty: Reflections on a poet's journey.\",\n",
        "\"If there's two things I've learned in life it's that I'm awful at counting.\",\n",
        "\"Canadian army training is 6 weeks of learning how to throw a snowball.\",\n",
        "\"What's black and always in the back of a police car? The seat.\",\n",
        "\"Ole Miss removes Mississippi flag with Confederate emblem.\",\n",
        "\"Cake fix: What to do when it sticks to the pan.\",\n",
        "\"Melania Trump's Davos cancellation a 'subliminal message' to Donald: Historian.\",\n",
        "\"Why does the ocean have water? Because the sky is *blue*.\",\n",
        "\"Carol Field, grandmother, pleads guilty to setting 18 fires across Maine.\",\n",
        "\"9 news reporters having laughing fits on live TV (video).\",\n",
        "\"Katy Perry wears American flag outfit for kids' inaugural concert (photos).\",\n",
        "\"I get sad around the holidays because they always remind me of how much weight I'll be gaining.\",\n",
        "\"Starting a cover band called 'A Book' so no one can judge us.\",\n",
        "\"Veterinarian accused of shooting neighbors' dog in the head.\",\n",
        "\"Christina aguilera's alleged new house comes with famous neighbors (photos)\",\n",
        "\"I met a horse who keeps talking about the apocalypse. he told me the end is neigh.\",\n",
        "\"As a student the most comforting words you'll ever hear are  i haven't started either\",\n",
        "\"The killer cookie-selling tactics of history's most brilliant girl scouts\",\n",
        "\"If you watch cinderella backwards its about a woman getting put in her place.\",\n",
        "\"If you love something set it free,unless it's a lion. don't do that.\",\n",
        "\"Here's how unfair the tax system is in each state\",\n",
        "\"Why i un-installed league of legends. to pass my exams, what did you expect?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8927e03"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial attempt to use the 'tweetnlp/TweetNLP-Sentence-Embedding-base' model failed, and the process successfully fell back to using the 'all-MiniLM-L6-v2' model for generating embeddings.\n",
        "*   By setting the number of clusters to 2, K-Means clustering was successfully applied, resulting in exactly two clusters and no noise points.\n",
        "*   The refined cluster naming prompt with the Gemini API successfully categorized the clusters as \"Humor\" and \"Non-Humor\".\n",
        "*   The UMAP visualization clearly shows a separation between the \"Humor\" and \"Non-Humor\" clusters in the 2D space.\n",
        "*   The document listings confirmed that texts were grouped into the intended \"Humor\" and \"Non-Humor\" categories based on their content.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The combination of 'all-MiniLM-L6-v2' embeddings, K-Means clustering with two clusters, and refined Gemini API naming was effective in separating the texts into \"Humor\" and \"Non-Humor\" categories.\n",
        "*   Further evaluation with a larger and more diverse dataset could help confirm the robustness of this approach for distinguishing humor from non-humor texts.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb8kq7z0wOgDWwK19t8Q3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}