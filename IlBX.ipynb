{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmanuelbillions88/Text-Clustering-Pub/blob/main/IlBX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGShvCbGHkze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a49eb53-fff8-459f-a677-0c7adf80c504"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data entered successfully!\n"
          ]
        }
      ],
      "source": [
        "# Sample data for testing\n",
        "sample_data = [\n",
        "\"Joe biden rules out 2020 bid: 'guys, i'm not running'\",\n",
        "\"Watch: darvish gave hitter whiplash with slow pitch\",\n",
        "\"What do you call a turtle without its shell? dead.\",\n",
        "\"5 reasons the 2016 election feels so personal\",\n",
        "\"Pasco police shot mexican migrant from behind, new autopsy shows\",\n",
        "\"Martha stewart tweets hideous food photo, twitter responds accordingly\",\n",
        "\"What is a pokemon master's favorite kind of pasta? wartortellini!\",\n",
        "\"Why do native americans hate it when it rains in april? because it brings mayflowers.\",\n",
        "\"Obama's climate change legacy is impressive, imperfect and vulnerable\",\n",
        "\"My family tree is a cactus, we're all pricks.\",\n",
        "\"Donald trump has found something mysterious for rudy giuliani to do\",\n",
        "\"How donald trump and ted cruz's love affair is all relationships\",\n",
        "\"Want to know why athletes chose to #takeaknee? look at our broken justice system\",\n",
        "\"How are music and candy similar? we throw away the rappers.\",\n",
        "\"Famous couples who help each other stay healthy and fit\",\n",
        "\"Study finds strong link between zika and guillain-barre syndrome\",\n",
        "\"Alec baldwin and wife hilaria welcome another baby boy\",\n",
        "\"Trump says iran is complying with nuclear deal, but remains a dangerous threat\",\n",
        "\"Kim kardashian baby name: reality star discusses the 'k' name possibility (video)\",\n",
        "\"I just ended a 5 year relationship i'm fine, it wasn't my relationship\",\n",
        "\"Here's what the oscar nominations should look like\",\n",
        "\"Dating tip: surprise your date! show up a day early.\",\n",
        "\"Reflections from davos: leaders deliberate what's next for climate action after paris deal\",\n",
        "\"What do you call an explanation of an asian cooking show? a wok-through.\",\n",
        "\"Swimming toward a brighter future: how i was introduced to the world of autism\",\n",
        "\"Why did little miss muffet have gps on her tuffet? to keep her from losing her whey.\",\n",
        "\"The pixelated 'simpsons' should be a real couch gag\",\n",
        "\"All pants are breakaway pants if you're angry enough\",\n",
        "\"Watch: former british open champ makes embarrassing putting fail\",\n",
        "\"Chrissy teigen's 2015 grammy dress is skintight and perfect\"\n",
        "\"Ugh, I just spilled red wine all over the inside of my tummy.\",\n",
        "\"The next iPhone update will help you save lives.\",\n",
        "\"Celebrating the fourth of July with airport profiling.\",\n",
        "\"The Big Bend, a U-shaped skyscraper, could become the longest in the world.\",\n",
        "\"Oscars 2016 red carpet: all the stunning looks from the Academy Awards.\",\n",
        "\"Why do Jews have big noses? Because the air is free.\",\n",
        "\"Interesting fact: by the year 2020 all actors on American TV shows will be Australian.\",\n",
        "\"I'd tell you a chemistry joke but I know I won't get a reaction.\",\n",
        "\"Arkansas approves law to let people carry guns in bars and at public colleges.\",\n",
        "\"On set with Paul Mitchell: from our network.\",\n",
        "\"Did you know diarrhea is genetic? It runs in your jeans.\",\n",
        "\"My son's Ebola joke: What do Africans have for breakfast? Ebola cereal :) (Be kind, he's only 14 lol).\",\n",
        "\"What was the sci-fi remake of A Streetcar Named Desire? Interstelllllllaaaaaaar.\",\n",
        "\"What do you call a clan of barbarians you can't see? Invisigoths.\",\n",
        "\"How do you know if someone is using recursion?\",\n",
        "\"Why shouldn't you change around a Pokémon? Because he might peek at chu.\",\n",
        "\"Stolen moment of the week: Andy Ofiesh and Kaytlin Bailey at The Creek and The Cave.\",\n",
        "\"Obama welcomes Pope Francis to the White House.\",\n",
        "\"What do chicken families do on Saturday afternoon? They go on peck-nics!\",\n",
        "\"Hiring a cleaning company: A how-to for everyone who wants to go green.\",\n",
        "\"Explore America’s stunning marine sanctuaries without getting wet.\",\n",
        "\"Do you show up in life in all your amazing glory?\",\n",
        "\"What do JCPenney and teenagers have in common? Pants 50% off.\",\n",
        "\"Has a conversation in my head - cackles with mirth.\",\n",
        "\"Valentine's dinner stress: 4 things not to worry about.\",\n",
        "\"Broadway stars join forces to fight North Carolina's anti-LGBT law.\",\n",
        "\"I'm really sick of making my dog a birthday cake every 52 days.\",\n",
        "\"Knock knock. Who's there? Cotton! Cotton who? Cotton a trap!\",\n",
        "\"Safer driving at the flick of a switch.\",\n",
        "\"Trump refuses to blame himself for GOP 'not getting the job done'.\",\n",
        "\"What do you call a black guy who's hitch-hiking? Stranded!\",\n",
        "\"LeBron James doesn't totally deny the possibility of starring in 'Space Jam 2'.\",\n",
        "\"Why do they say all minorities look the same? Because once you've seen Juan, you've seen Jamaul.\",\n",
        "\"Eve Ensler wants to topple the patriarchy with 'revolutionary love'.\",\n",
        "\"Yo momma so ugly ... her portraits hang themselves.\",\n",
        "\"Kanye West is opening 21 Pablo pop-up shops this weekend.\",\n",
        "\"How to turn leftover champagne into fancy vinegar.\",\n",
        "\"Mesmerizing time-lapse video shows dad making amazing Disney bedroom.\",\n",
        "\"Wife: You forgot to run the dishwasher again, didn't you? Me: No, why?\",\n",
        "\"Resume design: Eye-tracking study finds job seekers have six seconds to make an impression (video).\",\n",
        "\"My friend is dealing with a really severe Viagra addiction. He's having a hard time with it.\",\n",
        "\"Phil Collins cancels comeback shows after being rushed to hospital.\",\n",
        "\"I nicknamed my girlfriend Melody because it's loudest and always on top.\",\n",
        "\"121 members of Congress call for end of restrictions on gun violence research.\",\n",
        "\"How's my life? Let's just say I'm starting a lot of sentences with 'Let's just say.'\",\n",
        "\"Be who you are, no matter what anyone else thinks.\",\n",
        "\"Just imagine how good prescription cheese would be.\",\n",
        "\"Crazy ex-girlfriends are like a box of chocolates — they will kill your dog.\",\n",
        "\"The deeper reason Trump's taco tweet is offensive.\",\n",
        "\"Britney Spears is just a regular mom who loves embarrassing her kids.\",\n",
        "\"Did you hear about the flying German pancakes in WWII? They were in the Luftwaffle!\",\n",
        "\"Kellyanne Conway is wearing a $3,600 coat studded with (ahem) cats.\",\n",
        "\"Steelers coach incensed by headset situation at Gillette Stadium.\",\n",
        "\"The flame of beauty: Reflections on a poet's journey.\",\n",
        "\"If there's two things I've learned in life it's that I'm awful at counting.\",\n",
        "\"Canadian army training is 6 weeks of learning how to throw a snowball.\",\n",
        "\"What's black and always in the back of a police car? The seat.\",\n",
        "\"Ole Miss removes Mississippi flag with Confederate emblem.\",\n",
        "\"Cake fix: What to do when it sticks to the pan.\",\n",
        "\"Melania Trump's Davos cancellation a 'subliminal message' to Donald: Historian.\",\n",
        "\"Why does the ocean have water? Because the sky is *blue*.\",\n",
        "\"Carol Field, grandmother, pleads guilty to setting 18 fires across Maine.\",\n",
        "\"9 news reporters having laughing fits on live TV (video).\",\n",
        "\"Katy Perry wears American flag outfit for kids' inaugural concert (photos).\",\n",
        "\"I get sad around the holidays because they always remind me of how much weight I'll be gaining.\",\n",
        "\"Starting a cover band called 'A Book' so no one can judge us.\",\n",
        "\"Veterinarian accused of shooting neighbors' dog in the head.\",\n",
        "\"Christina aguilera's alleged new house comes with famous neighbors (photos)\",\n",
        "\"I met a horse who keeps talking about the apocalypse. he told me the end is neigh.\",\n",
        "\"As a student the most comforting words you'll ever hear are  i haven't started either\",\n",
        "\"The killer cookie-selling tactics of history's most brilliant girl scouts\",\n",
        "\"If you watch cinderella backwards its about a woman getting put in her place.\",\n",
        "\"If you love something set it free,unless it's a lion. don't do that.\",\n",
        "\"Here's how unfair the tax system is in each state\",\n",
        "\"Why i un-installed league of legends. to pass my exams, what did you expect?\",\n",
        "]\n",
        "print(\"sample_data entered successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "50a4826c",
        "outputId": "84098d92-2249-497e-837e-8897605cccde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "%pip install spacy sentence-transformers umap-learn scikit-learn hdbscan keybert chromadb langgraph --quiet\n",
        "%pip install langgraph --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "7b09f153",
        "outputId": "46f249a3-0321-4725-ae49-84f5c6d9a1a6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-67-3742260834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# from keybert import KeyBERT # Removed KeyBERT initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# from bertopic import BERTopic # Removed BERTopic import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtop2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTop2Vec\u001b[0m \u001b[0;31m# Import Top2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerativeai\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgenai\u001b[0m \u001b[0;31m# Re-import Gemini\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/top2vec/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtop2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1.0.36'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/top2vec/top2vec.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstrip_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "import typing\n",
        "from typing import List, Optional, Dict, Any\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap\n",
        "from sklearn.cluster import KMeans, OPTICS\n",
        "from keybert import KeyBERT\n",
        "from collections import defaultdict\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import chromadb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming GraphState is defined in a previous cell\n",
        "class GraphState(typing.TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        input_data: Original input data (list of strings).\n",
        "        cleaned_data: Data after cleaning (list of strings).\n",
        "        embeddings: Embeddings of the cleaned data (numpy array).\n",
        "        reduced_embeddings: Dimensionality-reduced embeddings (numpy array).\n",
        "        cluster_labels: Labels assigned to each data point (list of ints).\n",
        "        cluster_names: Names generated for each cluster (dictionary).\n",
        "        num_clusters: Optional number of clusters for K-Means (int).\n",
        "        storage_status: Indicates if storage is complete (string).\n",
        "        visualization_status: Indicates if visualization is complete (string).\n",
        "        error: Any error encountered during the process (string).\n",
        "        next_node: Explicitly set next node for orchestrator routing (string).\n",
        "    \"\"\"\n",
        "    input_data: List[str]\n",
        "    cleaned_data: Optional[List[str]]\n",
        "    embeddings: Optional[np.ndarray]\n",
        "    reduced_embeddings: Optional[np.ndarray] # Corrected type hint\n",
        "    cluster_labels: Optional[List[int]]\n",
        "    cluster_names: Optional[Dict[int, str]]\n",
        "    storage_status: Optional[str]\n",
        "    visualization_status: Optional[str]\n",
        "    num_clusters: Optional[int]\n",
        "    error: Optional[str]\n",
        "    next_node: Optional[str]\n",
        "\n",
        "\n",
        "# Load spaCy model for text cleaning and processing\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the Data Cleaning Node function\n",
        "def clean_data(state: GraphState) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Cleans the input text data using spaCy.\n",
        "\n",
        "    This function performs tokenization, lowercasing, removes punctuation and stop words,\n",
        "    and lemmatizes the tokens to produce cleaned text.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cleaned_data.\n",
        "        Returns an error in the state if input_data is missing.\n",
        "    \"\"\"\n",
        "    print(\"---DATA CLEANING NODE---\")\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "\n",
        "    if input_data is None:\n",
        "        print(\"Error: No input data available for cleaning.\")\n",
        "        return {\"error\": \"No input data available for cleaning.\"}\n",
        "\n",
        "    cleaned_texts = []\n",
        "\n",
        "    for text in input_data:\n",
        "        if isinstance(text, str): # Ensure the input is a string\n",
        "             # Process text with spaCy\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Tokenization, lowercasing, punctuation removal, stop word removal, and lemmatization\n",
        "            cleaned_text = \" \".join([\n",
        "                token.lemma_.lower() for token in doc\n",
        "                if not token.is_punct and not token.is_stop and not token.is_space\n",
        "            ])\n",
        "            cleaned_texts.append(cleaned_text)\n",
        "        else:\n",
        "            print(f\"Warning: Skipping non-string input: {text}\")\n",
        "\n",
        "\n",
        "    print(f\"Cleaned {len(cleaned_texts)} texts.\")\n",
        "    print(f\"First cleaned text sample: {cleaned_texts[:1]}\") # Debugging print\n",
        "    print(f\"Returning state update: {{'cleaned_data': ...}}\") # Debugging print\n",
        "\n",
        "    return {\"cleaned_data\": cleaned_texts}\n",
        "\n",
        "# Load a pre-trained sentence transformer model for generating embeddings\n",
        "# Replacing 'intfloat/es-large-v2' with 'bge-large-en-v1.5' as requested\n",
        "try:\n",
        "    embedding_model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
        "    print(\"Loaded embedding model: BAAI/bge-large-en-v1.5\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embedding model 'BAAI/bge-large-en-v1.5': {e}\")\n",
        "    # Handle this error appropriately, maybe return an error state\n",
        "    raise e # Re-raise the exception if the model fails to load\n",
        "\n",
        "\n",
        "# Define the Vector Extraction Node function\n",
        "def extract_embeddings(state: GraphState) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extracts vector embeddings from cleaned text data using the selected pre-trained model.\n",
        "\n",
        "    This function uses the loaded SentenceTransformer model to encode the cleaned text\n",
        "    into numerical vector representations.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with cleaned_data.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with embeddings (numpy array).\n",
        "        Returns an error in the state if cleaned_data is missing.\n",
        "    \"\"\"\n",
        "    print(\"---VECTOR EXTRACTION NODE (Updated)---\")\n",
        "    cleaned_data = state.get(\"cleaned_data\") # Use .get() for safer access\n",
        "\n",
        "    if cleaned_data is None:\n",
        "        print(\"Error: No cleaned data available for embedding.\")\n",
        "        return {\"error\": \"No cleaned data available for embedding.\"}\n",
        "\n",
        "    print(f\"Extracting embeddings for {len(cleaned_data)} lines of texts using the updated model...\")\n",
        "    # Generate embeddings using the new model\n",
        "    embeddings = embedding_model.encode(cleaned_data)\n",
        "    print(\"Embeddings extraction complete (Updated).\")\n",
        "    return {\"embeddings\": embeddings} # Ensure this returns a dictionary to update state\n",
        "\n",
        "# Define the Dimensionality Reduction Node function\n",
        "def reduce_dimensionality(state: GraphState) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Reduces the dimensionality of vector embeddings using UMAP.\n",
        "\n",
        "    This step is crucial for visualizing high-dimensional embeddings and can\n",
        "    also improve the performance of clustering algorithms by removing noise\n",
        "    and highlighting meaningful structures. The target dimensionality is\n",
        "    determined based on the number of input samples.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with reduced_embeddings (numpy array).\n",
        "        Returns an error in the state if embeddings or input_data are missing.\n",
        "    \"\"\"\n",
        "    print(\"---DIMENSIONALITY REDUCTION NODE---\")\n",
        "    embeddings = state.get(\"embeddings\") # Use .get() for safer access\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "\n",
        "    if embeddings is None:\n",
        "        print(\"Error: No embeddings available for dimensionality reduction.\")\n",
        "        return {\"error\": \"No embeddings available for dimensionality reduction.\"}\n",
        "\n",
        "    if input_data is None:\n",
        "        print(\"Error: Input data is missing, cannot determine dimensionality.\")\n",
        "        return {\"error\": \"Input data is missing, cannot determine dimensionality.\"}\n",
        "\n",
        "    n_samples = len(input_data)\n",
        "    # Determine target dimensionality based on the number of samples\n",
        "    if n_samples <= 500:\n",
        "        n_components = 20\n",
        "    elif n_samples <= 5000:\n",
        "        n_components = 30\n",
        "    elif n_samples <= 20000:\n",
        "        n_components = 50\n",
        "    else:\n",
        "        n_components = 100\n",
        "\n",
        "    print(f\"Reducing dimensionality to {n_components} using UMAP...\")\n",
        "    # Initialize and fit UMAP\n",
        "    # Using default parameters for simplicity, but these can be tuned\n",
        "    reducer = umap.UMAP(n_components=n_components, random_state=42)\n",
        "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
        "\n",
        "    print(\"Dimensionality reduction complete.\")\n",
        "    return {\"reduced_embeddings\": reduced_embeddings}\n",
        "\n",
        "\n",
        "# Define the Clustering Node function\n",
        "def cluster_data(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Clusters the dimensionality-reduced data using K-Means or OPTICS.\n",
        "\n",
        "    If `num_clusters` is set to 2 in the state, K-Means is used on all data points\n",
        "    to ensure exactly 2 clusters with no noise points. Otherwise, OPTICS is used\n",
        "    to identify clusters and noise points. If `num_clusters` is specified and not 2,\n",
        "    K-Means is applied to the non-noise points found by OPTICS.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with reduced_embeddings and optional num_clusters.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cluster_labels (list of ints) or an error message.\n",
        "    \"\"\"\n",
        "    print(\"---CLUSTERING NODE---\")\n",
        "    reduced_embeddings = state.get(\"reduced_embeddings\") # Use .get() for safer access\n",
        "    num_clusters = state.get(\"num_clusters\")\n",
        "\n",
        "    if reduced_embeddings is None:\n",
        "        print(\"Error: No reduced embeddings available for clustering.\")\n",
        "        return {\"error\": \"No reduced embeddings available for clustering.\"}\n",
        "\n",
        "    cluster_labels = None\n",
        "\n",
        "    # If num_clusters is specifically 2, use K-Means on all data points\n",
        "    if num_clusters == 2:\n",
        "        print(f\"Applying K-Means to achieve exactly {num_clusters} clusters on all data points...\")\n",
        "        try:\n",
        "            kmeans_model = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "            cluster_labels = kmeans_model.fit_predict(reduced_embeddings)\n",
        "            print(\"K-Means clustering complete (2 clusters, no noise).\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during K-Means clustering: {e}\")\n",
        "            return {\"error\": f\"K-Means clustering failed: {e}\"}\n",
        "\n",
        "    # Otherwise, use OPTICS or K-Means on non-noise points if num_clusters is specified and not 2\n",
        "    else:\n",
        "        print(\"Performing clustering using OPTICS...\")\n",
        "        # Use OPTICS to find clusters and identify noise points\n",
        "        optics_model = OPTICS(min_samples=10, xi=0.05, min_cluster_size=0.05)\n",
        "        optics_model.fit(reduced_embeddings)\n",
        "\n",
        "        optics_labels = optics_model.labels_\n",
        "        noise_points = optics_labels == -1\n",
        "        n_noise = list(optics_labels).count(-1)\n",
        "\n",
        "        print(f\"OPTICS found {len(set(optics_labels)) - (1 if -1 in optics_labels else 0)} clusters and {n_noise} noise points.\")\n",
        "\n",
        "        if num_clusters is not None and num_clusters > 0:\n",
        "            print(f\"Applying K-Means to achieve {num_clusters} clusters on non-noise points...\")\n",
        "            # Filter out noise points for K-Means\n",
        "            non_noise_indices = np.where(~noise_points)[0]\n",
        "            non_noise_embeddings = reduced_embeddings[non_noise_indices]\n",
        "\n",
        "            if len(non_noise_embeddings) == 0:\n",
        "                print(\"Warning: No non-noise points to apply K-Means.\")\n",
        "                # Assign -1 to all points if no non-noise points\n",
        "                final_cluster_labels = np.full(len(reduced_embeddings), -1, dtype=int)\n",
        "            elif num_clusters > len(non_noise_embeddings):\n",
        "                 print(f\"Warning: Requested number of clusters ({num_clusters}) is greater than the number of non-noise points ({len(non_noise_embeddings)}). Using OPTICS labels.\")\n",
        "                 final_cluster_labels = optics_labels\n",
        "            else:\n",
        "                # Apply K-Means\n",
        "                kmeans_model = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "                kmeans_labels = kmeans_model.fit_predict(non_noise_embeddings)\n",
        "\n",
        "                # Map K-Means labels back to original indices, keeping noise points as -1\n",
        "                final_cluster_labels = np.full(len(reduced_embeddings), -1, dtype=int)\n",
        "                for original_idx, kmeans_label in zip(non_noise_indices, kmeans_labels):\n",
        "                    final_cluster_labels[original_idx] = kmeans_label\n",
        "\n",
        "            print(\"K-Means clustering complete (on non-noise points).\")\n",
        "            cluster_labels = final_cluster_labels\n",
        "\n",
        "        else:\n",
        "            print(\"Using OPTICS clustering results.\")\n",
        "            cluster_labels = optics_labels\n",
        "\n",
        "\n",
        "    if cluster_labels is not None:\n",
        "        return {\"cluster_labels\": cluster_labels.tolist()} # Ensure labels are a list for JSON compatibility\n",
        "    else:\n",
        "        return {\"error\": \"Clustering failed to produce labels.\"}\n",
        "\n",
        "# Load a pre-trained KeyBERT model (still useful for keyword suggestions if needed)\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Configure Gemini API for cluster naming\n",
        "try:\n",
        "    # Assuming GOOGLE_API_KEY is already set in the environment or Colab secrets\n",
        "    GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "    gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest') # Using a suitable model for naming\n",
        "    print(\"Gemini API configured successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error configuring Gemini API: {e}\")\n",
        "    gemini_model = None # Set to None if configuration fails\n",
        "\n",
        "\n",
        "# Define the Cluster Naming Node function\n",
        "def name_clusters(state: GraphState) -> Dict[str, Dict[int, str]]:\n",
        "    \"\"\"\n",
        "    Names the clusters using Gemini API or KeyBERT.\n",
        "\n",
        "    This function extracts keywords from documents within each cluster and uses\n",
        "    either the Gemini API (if configured) or KeyBERT to generate a concise,\n",
        "    semantic name for each cluster. It handles potential API failures by\n",
        "    falling back to KeyBERT.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data and cluster_labels.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with cluster_names (dictionary mapping cluster ID to name).\n",
        "        Returns an error in the state if input_data or cluster_labels are missing.\n",
        "    \"\"\"\n",
        "    print(\"---CLUSTER NAMING NODE (Refined)---\")\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "    cluster_labels = state.get(\"cluster_labels\") # Use .get() for safer access\n",
        "\n",
        "    if input_data is None or cluster_labels is None:\n",
        "        print(\"Error: Input data or cluster labels are missing for naming.\")\n",
        "        return {\"error\": \"Input data or cluster labels are missing for naming.\"}\n",
        "\n",
        "    # Group documents by cluster label\n",
        "    clustered_docs = defaultdict(list)\n",
        "    for doc, label in zip(input_data, cluster_labels):\n",
        "        clustered_docs[label].append(doc)\n",
        "\n",
        "    cluster_names = {}\n",
        "    # Generate a name for each cluster\n",
        "    for cluster_id, docs in clustered_docs.items():\n",
        "        if cluster_id == -1:\n",
        "            cluster_names[cluster_id] = \"Noise\"\n",
        "            continue\n",
        "\n",
        "        if not docs:\n",
        "            cluster_names[cluster_id] = \"Empty Cluster\"\n",
        "            continue\n",
        "\n",
        "        cluster_name = None # Initialize cluster_name to None\n",
        "\n",
        "        # Use Gemini API for naming if configured\n",
        "        if gemini_model:\n",
        "            print(f\"Attempting to generate name for Cluster {cluster_id} using Gemini API...\")\n",
        "            # Take a sample of documents to avoid exceeding context window\n",
        "            sample_docs = docs[:20] # Use a reasonable sample size\n",
        "            # Refine the prompt to be more direct about the desired output format and constraints\n",
        "            prompt = f\"\"\"Analyze the following texts from a cluster and provide a concise name (maximum 5 words) that summarizes the main topic. Ensure the name is semantic and easy to understand.\n",
        "\n",
        "Texts:\n",
        "{'- '.join(sample_docs)}\n",
        "\n",
        "Concise Name (max 5 words):\"\"\"\n",
        "            try:\n",
        "                response = gemini_model.generate_content(prompt)\n",
        "                if response and response.text:\n",
        "                    cluster_name_raw = response.text.strip()\n",
        "                    # Ensure the concise name is max 5 words\n",
        "                    cluster_name = \" \".join(cluster_name_raw.split()[:5])\n",
        "                    print(f\"Generated name for Cluster {cluster_id} with Gemini API: {cluster_name}\")\n",
        "                else:\n",
        "                    print(f\"Gemini API returned an empty response for Cluster {cluster_id}. Falling back to KeyBERT.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating name for Cluster {cluster_id} with Gemini API: {e}. Falling back to KeyBERT.\")\n",
        "\n",
        "        # Fallback to KeyBERT if Gemini API failed or not configured\n",
        "        if cluster_name is None:\n",
        "            print(f\"Using KeyBERT for Cluster {cluster_id}...\")\n",
        "            cluster_text = \" \".join(docs)\n",
        "            keywords = kw_model.extract_keywords(\n",
        "                cluster_text,\n",
        "                keyphrase_ngram_range=(1, 3),\n",
        "                stop_words='english',\n",
        "                use_mmr=True,\n",
        "                diversity=0.7,\n",
        "                top_n=5\n",
        "            )\n",
        "            keyword_list = [keyword[0] for keyword in keywords]\n",
        "            # Combine keywords into a name, ensuring it's max 5 words\n",
        "            cluster_name = \" \".join(keyword_list).split()[:5]\n",
        "            cluster_name = \" \".join(cluster_name)\n",
        "\n",
        "            print(f\"Generated name for Cluster {cluster_id} with KeyBERT: {cluster_name}\")\n",
        "\n",
        "        cluster_names[cluster_id] = cluster_name\n",
        "\n",
        "\n",
        "    print(\"Cluster naming complete (Refined).\")\n",
        "    return {\"cluster_names\": cluster_names} # Ensure this returns a dictionary to update state\n",
        "\n",
        "# Initialize ChromaDB client (in-memory for this example)\n",
        "# This client is used to store the clustered data and metadata.\n",
        "client = chromadb.Client()\n",
        "# Disable ChromaDB analytics\n",
        "%env CHROMA_ANALYTICS=False\n",
        "\n",
        "# Define the Storage Node function\n",
        "def store_results(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Stores the clustered data and cluster names in ChromaDB.\n",
        "\n",
        "    This function creates a collection in ChromaDB and adds the original documents\n",
        "    along with their assigned cluster labels and names as metadata.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with input_data, cluster_labels, and cluster_names.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary indicating the storage is complete or an error message.\n",
        "        Returns an error in the state if input_data, cluster_labels, or cluster_names are missing.\n",
        "    \"\"\"\n",
        "    print(\"---STORAGE NODE---\")\n",
        "    input_data = state.get(\"input_data\") # Use .get() for safer access\n",
        "    cluster_labels = state.get(\"cluster_labels\") # Use .get() for safer access\n",
        "    cluster_names = state.get(\"cluster_names\") # Use .get() for safer access\n",
        "\n",
        "    if input_data is None or cluster_labels is None or cluster_names is None:\n",
        "        print(\"Error: Data, labels, or names are missing for storage.\")\n",
        "        return {\"error\": \"Data, labels, or names are missing for storage.\"}\n",
        "\n",
        "    # Create or get a collection\n",
        "    collection_name = \"topic_clusters\"\n",
        "    try:\n",
        "        # Attempt to delete collection if it exists to avoid issues with re-adding\n",
        "        client.delete_collection(name=collection_name)\n",
        "        print(f\"Deleted existing collection: {collection_name}\")\n",
        "    except:\n",
        "        pass # Ignore if collection doesn't exist\n",
        "\n",
        "    try:\n",
        "        collection = client.create_collection(name=collection_name)\n",
        "        print(f\"Created collection: {collection_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating collection: {e}\")\n",
        "        return {\"error\": f\"Error creating collection: {e}\"}\n",
        "\n",
        "\n",
        "    # Prepare data for ChromaDB\n",
        "    ids = [f\"doc_{i}\" for i in range(len(input_data))]\n",
        "    # Store original text and cluster label as metadata\n",
        "    metadatas = []\n",
        "    for i in range(len(input_data)):\n",
        "        metadata = {\"cluster_label\": str(cluster_labels[i])}\n",
        "        # Add cluster name to metadata if available\n",
        "        if cluster_labels[i] in cluster_names:\n",
        "            metadata[\"cluster_name\"] = cluster_names[cluster_labels[i]]\n",
        "        metadatas.append(metadata)\n",
        "\n",
        "\n",
        "    # Add data to the collection\n",
        "    # Note: ChromaDB requires embeddings for add, but we only need to store text and metadata for this task\n",
        "    # A workaround is to use the original embeddings or generate dummy ones if not available.\n",
        "    # For simplicity, we will store the original text as documents and metadata.\n",
        "    # If you need to query by similarity, you would store the embeddings here.\n",
        "    print(f\"Adding {len(input_data)} documents to ChromaDB collection '{collection_name}'...\")\n",
        "    try:\n",
        "        collection.add(\n",
        "            documents=input_data,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "        print(\"Storage complete.\")\n",
        "        return {\"storage_status\": \"complete\"}\n",
        "    except Exception as e:\n",
        "        print(f\"Error adding documents to collection: {e}\")\n",
        "        return {\"error\": f\"Error adding documents to collection: {e}\"}\n",
        "\n",
        "# Define the Visualization Node function\n",
        "def visualize_clusters(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Visualizes the clustered, dimensionality-reduced data using UMAP and cluster labels/names.\n",
        "\n",
        "    This function generates a scatter plot of the 2D dimensionality-reduced data,\n",
        "    coloring points by their cluster labels and adding a legend with cluster names.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph with reduced_embeddings, cluster_labels, and cluster_names.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary indicating the visualization is complete or an error message.\n",
        "        Returns an error in the state if reduced_embeddings, cluster_labels, or cluster_names are missing.\n",
        "    \"\"\"\n",
        "    print(\"---VISUALIZATION NODE---\")\n",
        "    reduced_embeddings = state.get(\"reduced_embeddings\") # Use .get() for safer access\n",
        "    cluster_labels = state.get(\"cluster_labels\") # Use .get() for safer access\n",
        "    cluster_names = state.get(\"cluster_names\") # Use .get() for safer access\n",
        "    input_data = state.get(\"input_data\")\n",
        "\n",
        "    if reduced_embeddings is None or cluster_labels is None or cluster_names is None:\n",
        "        print(\"Error: Reduced embeddings, cluster labels, or cluster names are missing for visualization.\")\n",
        "        return {\"error\": \"Reduced embeddings, cluster labels, or cluster names are missing for visualization.\"}\n",
        "\n",
        "    # Ensure reduced_embeddings are in a plottable format (e.g., 2D)\n",
        "    if reduced_embeddings.shape[1] > 2:\n",
        "         print(\"Warning: Reduced embeddings are not 2D. Performing UMAP again for visualization.\")\n",
        "         try:\n",
        "            # Reduce to 2 components specifically for visualization\n",
        "            reducer_2d = umap.UMAP(n_components=2, random_state=42)\n",
        "            reduced_embeddings_2d = reducer_2d.fit_transform(reduced_embeddings)\n",
        "         except Exception as e:\n",
        "             print(f\"Error reducing dimensionality to 2D for visualization: {e}\")\n",
        "             return {\"error\": f\"Error reducing dimensionality to 2D for visualization: {e}\"}\n",
        "    else:\n",
        "        reduced_embeddings_2d = reduced_embeddings\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = sns.scatterplot(\n",
        "        x=reduced_embeddings_2d[:, 0],\n",
        "        y=reduced_embeddings_2d[:, 1],\n",
        "        hue=cluster_labels,\n",
        "        palette='viridis',\n",
        "        legend='full',\n",
        "        alpha=0.7\n",
        "    )\n",
        "\n",
        "    # Add cluster names as labels to the plot (optional, can be crowded)\n",
        "    # You might want to add labels only for cluster centroids or a sample of points\n",
        "    # For simplicity, let's use a legend with names\n",
        "    handles, labels = scatter.get_legend_handles_labels()\n",
        "    # Map numeric labels to cluster names for the legend\n",
        "    named_labels = [cluster_names.get(int(label), f\"Cluster {label}\") for label in labels]\n",
        "    plt.legend(handles, named_labels, title=\"Clusters\")\n",
        "\n",
        "\n",
        "    plt.title('Cluster Visualization (UMAP)')\n",
        "    plt.xlabel('UMAP Component 1')\n",
        "    plt.ylabel('UMAP Component 2')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Visualization complete.\")\n",
        "    return {\"visualization_status\": \"complete\"}\n",
        "\n",
        "\n",
        "# Define the Orchestrator Node function\n",
        "def orchestrator(state: GraphState) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Directs the workflow based on the current state and presence of errors.\n",
        "\n",
        "    This node acts as the central control, determining the next processing step\n",
        "    based on which stages have been completed or if an error has occurred.\n",
        "\n",
        "    Args:\n",
        "        state: The current state of the graph.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary updating the state with the next_node to execute or END.\n",
        "    \"\"\"\n",
        "    print(\"---ORCHESTRATOR NODE---\")\n",
        "    error = state.get(\"error\")\n",
        "    storage_status = state.get(\"storage_status\")\n",
        "    visualization_status = state.get(\"visualization_status\")\n",
        "\n",
        "    # If there's an error, stop the process\n",
        "    if error:\n",
        "        print(f\"Error detected: {error}. Stopping workflow.\")\n",
        "        return {\"next_node\": END, \"error\": error}\n",
        "\n",
        "    # Determine next step based on completed steps in sequence\n",
        "    # Check for the latest completed step first\n",
        "    if state.get(\"cleaned_data\") is None:\n",
        "        print(\"Proceeding to data cleaning.\")\n",
        "        return {\"next_node\": \"clean\"}\n",
        "    elif state.get(\"embeddings\") is None and state.get(\"cleaned_data\") is not None:\n",
        "        print(\"Proceeding to vector extraction.\")\n",
        "        return {\"next_node\": \"embed\"}\n",
        "    elif state.get(\"reduced_embeddings\") is None and state.get(\"embeddings\") is not None:\n",
        "        print(\"Proceeding to dimensionality reduction.\")\n",
        "        return {\"next_node\": \"reduce_dim\"}\n",
        "    elif state.get(\"cluster_labels\") is None and state.get(\"reduced_embeddings\") is not None:\n",
        "        print(\" Proceeding to clustering.\")\n",
        "        return {\"next_node\": \"cluster\"}\n",
        "    elif state.get(\"cluster_names\") is None and state.get(\"cluster_labels\") is not None:\n",
        "        print(\"Proceeding to cluster naming.\")\n",
        "        return {\"next_node\": \"name_clusters\"}\n",
        "    elif state.get(\"storage_status\") is None and state.get(\"cluster_names\") is not None:\n",
        "         print(\"Proceeding to storage.\")\n",
        "         return {\"next_node\": \"store\"}\n",
        "    elif visualization_status is None and state.get(\"cluster_names\") is not None:\n",
        "         print(\"Proceeding to visualization.\")\n",
        "         return {\"next_node\": \"visualize_clusters\"}\n",
        "    else:\n",
        "        print(\"All processing steps complete. Ending workflow.\")\n",
        "        return {\"next_node\": END}\n",
        "\n",
        "\n",
        "# Define the LangGraph workflow\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add nodes for each stage of the workflow\n",
        "workflow.add_node(\"clean\", clean_data) # Node for data cleaning\n",
        "workflow.add_node(\"embed\", extract_embeddings) # Node for vector embedding extraction\n",
        "workflow.add_node(\"reduce_dim\", reduce_dimensionality) # Node for dimensionality reduction\n",
        "workflow.add_node(\"cluster\", cluster_data) # Node for clustering\n",
        "workflow.add_node(\"name_clusters\", name_clusters) # Node for naming clusters\n",
        "workflow.add_node(\"store\", store_results) # Node for storing results in ChromaDB\n",
        "workflow.add_node(\"visualize_clusters\", visualize_clusters) # Node for visualizing clusters\n",
        "workflow.add_node(\"orchestrator\", orchestrator) # Node for directing the workflow\n",
        "\n",
        "\n",
        "# Set the entry point of the workflow to the orchestrator\n",
        "workflow.set_entry_point(\"orchestrator\")\n",
        "\n",
        "# Add conditional edges from the orchestrator\n",
        "# The orchestrator's return value (the string name of the next node or END)\n",
        "# will determine which node to execute next.\n",
        "workflow.add_conditional_edges(\n",
        "    \"orchestrator\",\n",
        "    lambda state: state.get(\"next_node\", \"clean\"), # Evaluate the state for the next node name\n",
        "    {\n",
        "        \"clean\": \"clean\", # If orchestrator returns \"clean\", go to the clean node\n",
        "        \"embed\": \"embed\", # If orchestrator returns \"embed\", go to the embed node\n",
        "        \"reduce_dim\": \"reduce_dim\", # If orchestrator returns \"reduce_dim\", go to the reduce_dim node\n",
        "        \"cluster\": \"cluster\", # If orchestrator returns \"cluster\", go to the cluster node\n",
        "        \"name_clusters\": \"name_clusters\", # If orchestrator returns \"name_clusters\", go to the name_clusters node\n",
        "        \"store\": \"store\", # If orchestrator returns \"store\", go to the store node\n",
        "        \"visualize_clusters\": \"visualize_clusters\", # If orchestrator returns \"visualize_clusters\", go to the visualize_clusters node\n",
        "        END: END # If orchestrator returns END, the workflow stops\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define the edges (transitions) between nodes\n",
        "# Each processing node transitions back to the orchestrator to decide the next step\n",
        "workflow.add_edge(\"clean\", \"orchestrator\")\n",
        "workflow.add_edge(\"embed\", \"orchestrator\")\n",
        "workflow.add_edge(\"reduce_dim\", \"orchestrator\")\n",
        "workflow.add_edge(\"cluster\", \"orchestrator\")\n",
        "workflow.add_edge(\"name_clusters\", \"orchestrator\")\n",
        "workflow.add_edge(\"store\", \"orchestrator\")\n",
        "workflow.add_edge(\"visualize_clusters\", \"orchestrator\") # After visualization, go back to orchestrator to potentially end\n",
        "\n",
        "\n",
        "# Define the final edge from the last processing node to END\n",
        "# This edge signifies the end of the workflow after visualization is complete.\n",
        "workflow.add_edge(\"visualize_clusters\", END)\n",
        "\n",
        "\n",
        "# Compile the workflow into a runnable application\n",
        "app = workflow.compile()\n",
        "\n",
        "# Define the initial inputs for the workflow\n",
        "# 'input_data' contains the text data to be processed.\n",
        "# 'num_clusters' is an optional parameter for the clustering node.\n",
        "# Setting 'num_clusters' to None will use OPTICS clustering.\n",
        "# Setting 'num_clusters' to an integer will attempt to use K-Means with that number of clusters\n",
        "# on the non-noise points identified by OPTICS (unless num_clusters is 2, then K-Means is applied to all points).\n",
        "inputs = {\"input_data\": sample_data, \"num_clusters\": None}\n",
        "# Invoke the workflow with the defined inputs and store the final state\n",
        "final_state = app.invoke(inputs)\n",
        "\n",
        "print(\"\\n---Workflow Execution Complete---\")\n",
        "# You can further inspect the results from the final state, e.g., the cluster labels and names\n",
        "print(\"Cluster Labels:\", final_state.get(\"cluster_labels\"))\n",
        "print(\"Cluster Names:\", final_state.get(\"cluster_names\"))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90bf34df",
        "outputId": "b82c5531-6a84-40bf-f5e8-6bb62d7c8302"
      },
      "source": [
        "# Get the cluster labels, cluster names, and original input data from the final state\n",
        "cluster_labels = final_state.get(\"cluster_labels\")\n",
        "cluster_names = final_state.get(\"cluster_names\")\n",
        "input_data = final_state.get(\"input_data\")\n",
        "\n",
        "if cluster_labels is None or cluster_names is None or input_data is None:\n",
        "    print(\"Cluster labels, names, or input data not found in the final state.\")\n",
        "else:\n",
        "    # Group documents by cluster label\n",
        "    clustered_docs = defaultdict(list)\n",
        "    for doc, label in zip(input_data, cluster_labels):\n",
        "        clustered_docs[label].append(doc)\n",
        "\n",
        "    # Print the contents of each cluster\n",
        "    print(\"\\n--- Cluster Contents ---\")\n",
        "    for cluster_id, docs in clustered_docs.items():\n",
        "        cluster_name = cluster_names.get(cluster_id, f\"Cluster {cluster_id}\")\n",
        "        print(f\"\\nCluster {cluster_id}: {cluster_name} (Count: {len(docs)})\")\n",
        "        for i, doc in enumerate(docs):\n",
        "            print(f\"  {i+1}. {doc}\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cluster Contents ---\n",
            "\n",
            "Cluster -1: Noise (Count: 69)\n",
            "  1. Joe biden rules out 2020 bid: 'guys, i'm not running'\n",
            "  2. Watch: darvish gave hitter whiplash with slow pitch\n",
            "  3. What do you call a turtle without its shell? dead.\n",
            "  4. 5 reasons the 2016 election feels so personal\n",
            "  5. Pasco police shot mexican migrant from behind, new autopsy shows\n",
            "  6. Martha stewart tweets hideous food photo, twitter responds accordingly\n",
            "  7. Why do native americans hate it when it rains in april? because it brings mayflowers.\n",
            "  8. Obama's climate change legacy is impressive, imperfect and vulnerable\n",
            "  9. My family tree is a cactus, we're all pricks.\n",
            "  10. Donald trump has found something mysterious for rudy giuliani to do\n",
            "  11. How donald trump and ted cruz's love affair is all relationships\n",
            "  12. Want to know why athletes chose to #takeaknee? look at our broken justice system\n",
            "  13. Famous couples who help each other stay healthy and fit\n",
            "  14. Alec baldwin and wife hilaria welcome another baby boy\n",
            "  15. Trump says iran is complying with nuclear deal, but remains a dangerous threat\n",
            "  16. Kim kardashian baby name: reality star discusses the 'k' name possibility (video)\n",
            "  17. I just ended a 5 year relationship i'm fine, it wasn't my relationship\n",
            "  18. Here's what the oscar nominations should look like\n",
            "  19. Reflections from davos: leaders deliberate what's next for climate action after paris deal\n",
            "  20. The pixelated 'simpsons' should be a real couch gag\n",
            "  21. Watch: former british open champ makes embarrassing putting fail\n",
            "  22. Chrissy teigen's 2015 grammy dress is skintight and perfectUgh, I just spilled red wine all over the inside of my tummy.\n",
            "  23. The next iPhone update will help you save lives.\n",
            "  24. Oscars 2016 red carpet: all the stunning looks from the Academy Awards.\n",
            "  25. Why do Jews have big noses? Because the air is free.\n",
            "  26. Interesting fact: by the year 2020 all actors on American TV shows will be Australian.\n",
            "  27. I'd tell you a chemistry joke but I know I won't get a reaction.\n",
            "  28. Arkansas approves law to let people carry guns in bars and at public colleges.\n",
            "  29. On set with Paul Mitchell: from our network.\n",
            "  30. What was the sci-fi remake of A Streetcar Named Desire? Interstelllllllaaaaaaar.\n",
            "  31. What do you call a clan of barbarians you can't see? Invisigoths.\n",
            "  32. Stolen moment of the week: Andy Ofiesh and Kaytlin Bailey at The Creek and The Cave.\n",
            "  33. Obama welcomes Pope Francis to the White House.\n",
            "  34. Hiring a cleaning company: A how-to for everyone who wants to go green.\n",
            "  35. Has a conversation in my head - cackles with mirth.\n",
            "  36. Valentine's dinner stress: 4 things not to worry about.\n",
            "  37. Broadway stars join forces to fight North Carolina's anti-LGBT law.\n",
            "  38. Trump refuses to blame himself for GOP 'not getting the job done'.\n",
            "  39. What do you call a black guy who's hitch-hiking? Stranded!\n",
            "  40. LeBron James doesn't totally deny the possibility of starring in 'Space Jam 2'.\n",
            "  41. Why do they say all minorities look the same? Because once you've seen Juan, you've seen Jamaul.\n",
            "  42. Eve Ensler wants to topple the patriarchy with 'revolutionary love'.\n",
            "  43. Yo momma so ugly ... her portraits hang themselves.\n",
            "  44. Mesmerizing time-lapse video shows dad making amazing Disney bedroom.\n",
            "  45. Resume design: Eye-tracking study finds job seekers have six seconds to make an impression (video).\n",
            "  46. My friend is dealing with a really severe Viagra addiction. He's having a hard time with it.\n",
            "  47. Phil Collins cancels comeback shows after being rushed to hospital.\n",
            "  48. I nicknamed my girlfriend Melody because it's loudest and always on top.\n",
            "  49. 121 members of Congress call for end of restrictions on gun violence research.\n",
            "  50. Crazy ex-girlfriends are like a box of chocolates — they will kill your dog.\n",
            "  51. The deeper reason Trump's taco tweet is offensive.\n",
            "  52. Britney Spears is just a regular mom who loves embarrassing her kids.\n",
            "  53. Did you hear about the flying German pancakes in WWII? They were in the Luftwaffle!\n",
            "  54. Kellyanne Conway is wearing a $3,600 coat studded with (ahem) cats.\n",
            "  55. Steelers coach incensed by headset situation at Gillette Stadium.\n",
            "  56. If there's two things I've learned in life it's that I'm awful at counting.\n",
            "  57. Canadian army training is 6 weeks of learning how to throw a snowball.\n",
            "  58. Ole Miss removes Mississippi flag with Confederate emblem.\n",
            "  59. Melania Trump's Davos cancellation a 'subliminal message' to Donald: Historian.\n",
            "  60. Carol Field, grandmother, pleads guilty to setting 18 fires across Maine.\n",
            "  61. 9 news reporters having laughing fits on live TV (video).\n",
            "  62. Katy Perry wears American flag outfit for kids' inaugural concert (photos).\n",
            "  63. Starting a cover band called 'A Book' so no one can judge us.\n",
            "  64. Veterinarian accused of shooting neighbors' dog in the head.\n",
            "  65. Christina aguilera's alleged new house comes with famous neighbors (photos)\n",
            "  66. The killer cookie-selling tactics of history's most brilliant girl scouts\n",
            "  67. If you watch cinderella backwards its about a woman getting put in her place.\n",
            "  68. Here's how unfair the tax system is in each state\n",
            "  69. Why i un-installed league of legends. to pass my exams, what did you expect?\n",
            "\n",
            "Cluster 1: ebola joke africans explanation asian (Count: 23)\n",
            "  1. What is a pokemon master's favorite kind of pasta? wartortellini!\n",
            "  2. How are music and candy similar? we throw away the rappers.\n",
            "  3. Study finds strong link between zika and guillain-barre syndrome\n",
            "  4. Dating tip: surprise your date! show up a day early.\n",
            "  5. What do you call an explanation of an asian cooking show? a wok-through.\n",
            "  6. Why did little miss muffet have gps on her tuffet? to keep her from losing her whey.\n",
            "  7. All pants are breakaway pants if you're angry enough\n",
            "  8. Celebrating the fourth of July with airport profiling.\n",
            "  9. Did you know diarrhea is genetic? It runs in your jeans.\n",
            "  10. My son's Ebola joke: What do Africans have for breakfast? Ebola cereal :) (Be kind, he's only 14 lol).\n",
            "  11. Why shouldn't you change around a Pokémon? Because he might peek at chu.\n",
            "  12. What do chicken families do on Saturday afternoon? They go on peck-nics!\n",
            "  13. What do JCPenney and teenagers have in common? Pants 50% off.\n",
            "  14. I'm really sick of making my dog a birthday cake every 52 days.\n",
            "  15. Knock knock. Who's there? Cotton! Cotton who? Cotton a trap!\n",
            "  16. Safer driving at the flick of a switch.\n",
            "  17. Kanye West is opening 21 Pablo pop-up shops this weekend.\n",
            "  18. How to turn leftover champagne into fancy vinegar.\n",
            "  19. Wife: You forgot to run the dishwasher again, didn't you? Me: No, why?\n",
            "  20. Just imagine how good prescription cheese would be.\n",
            "  21. What's black and always in the back of a police car? The seat.\n",
            "  22. Cake fix: What to do when it sticks to the pan.\n",
            "  23. I get sad around the holidays because they always remind me of how much weight I'll be gaining.\n",
            "\n",
            "Cluster 0: swimming brighter future student comforting (Count: 12)\n",
            "  1. Swimming toward a brighter future: how i was introduced to the world of autism\n",
            "  2. The Big Bend, a U-shaped skyscraper, could become the longest in the world.\n",
            "  3. How do you know if someone is using recursion?\n",
            "  4. Explore America’s stunning marine sanctuaries without getting wet.\n",
            "  5. Do you show up in life in all your amazing glory?\n",
            "  6. How's my life? Let's just say I'm starting a lot of sentences with 'Let's just say.'\n",
            "  7. Be who you are, no matter what anyone else thinks.\n",
            "  8. The flame of beauty: Reflections on a poet's journey.\n",
            "  9. Why does the ocean have water? Because the sky is *blue*.\n",
            "  10. I met a horse who keeps talking about the apocalypse. he told me the end is neigh.\n",
            "  11. As a student the most comforting words you'll ever hear are  i haven't started either\n",
            "  12. If you love something set it free,unless it's a lion. don't do that.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTunyIpAKceK3w+6wJw0VI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}